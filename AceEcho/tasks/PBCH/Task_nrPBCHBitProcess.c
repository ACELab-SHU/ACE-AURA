/**
 * ****************************************
 * @file        Task_nrPBCHBitProcess.c
 * @brief       PBCH bit process
 * @author      yuanfeng
 * @date        2024.6.26
 * @copyright   ACE-Lab(Shanghai University)
 * ****************************************
 */

#include "riscv_printf.h"
#include "venus.h"

typedef short __v2048i16 __attribute__((ext_vector_type(2048)));
typedef char  __v4096i8 __attribute__((ext_vector_type(4096)));

#define INF 0x7F
// input data for test

//---------------------------rateMatching------------------------------------//
// uint16_t pi_[32] = {0,  1,  2,  4,  3,  5,  6,  7,  8,  16, 9,
//                     17, 10, 18, 11, 19, 12, 20, 13, 21, 14, 22,
//                     15, 23, 24, 25, 26, 28, 27, 29, 30, 31};

//---------------------------------------------------------------//

//---------------------------CRC------------------------------------//
enum crcType { CRC24A, CRC24B, CRC24C, CRC16, CRC11, CRC6 };

// uint8_t POLY_CRC24A[25] = {1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
//                            1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1};
// uint8_t POLY_CRC24B[25] = {1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
//                            0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1};
// uint8_t POLY_CRC24C[25] = {1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
//                            0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1};
// uint8_t POLY_CRC16[17] = {1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1};
// uint8_t POLY_CRC11[12] = {1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1};
// uint8_t POLY_CRC6[7] = {1, 1, 0, 0, 0, 0, 1};
//---------------------------------------------------------------//
//---------------------------polar------------------------------------//
// short input_llr[864] = {
//     -46, -42, -49, -57, -57, 43,  44,  44,  -47, -46, -49, -44, -41, 55, -44,
//     -58, -40, 41,  49,  40,  -44, 55,  47,  -40, 50,  41,  -41, 50,  56,  52,
//     49,  55,  -46, -52, 45,  49,  47,  -45, -46, -47, -46, 42,  43,  -42, 39,
//     49,  48,  -45, -40, -45, -45, 38,  -37, 42,  42,  -50, 47,  51,  45, -52,
//     45,  -48, 52,  -42, -44, 54,  -42, -43, 51,  -48, -40, 49,  37,  -41, 44,
//     54,  -41, 50,  -44, 36,  -42, 35,  46,  -40, -39, 41,  42,  44,  -44,
//     -54, 46,  52,  -46, -50, -44, 40,  -40, -49, 41,  -45, 50,  -63, -50, 40,
//     51, -49, -54, -51, -42, -56, -44, -47, -43, 46,  -52, 43,  42,  -48, -53,
//     -43, -46, 52,  -45, 44,  43,  -47, 47,  40,  -47, -42, -47, -54, -48, 40,
//     48, 40,  -45, -40, -50, -58, -43, 56,  -46, -47, -52, -47, -48, -45, -49,
//     45, 50,  50,  -42, -39, -46, -41, -37, 37,  -32, -59, -50, 39,  47,  38,
//     -49, 38,  48,  -44, 63,  44,  -50, 50,  44,  48,  40,  43,  -46, 50,  40,
//     46, -48, 47,  48,  -44, 56,  50,  -52, 37,  51,  52,  48,  56,  -46, -47,
//     51, 48,  50,  -45, -53, -52, -48, 47,  56,  -53, 48,  47,  52,  -46, -41,
//     -42, 49,  45,  49,  -41, -43, -54, -54, 42,  49,  -43, 51,  41,  41, -50,
//     -43, 40,  -45, -47, 43,  -42, -35, 43,  45,  -46, 36,  36,  -39, 37, -39,
//     40, -45, 44,  -46, -49, 47,  -50, -52, 41,  46,  -47, 41,  51,  -42, 47,
//     -47, 48,  -42, -60, -49, 38,  -51, 50,  49,  -43, 47,  51,  46,  -44, 49,
//     -47, 37,  -50, -48, -46, -42, 39,  -52, 37,  53,  -52, 41,  57,  45, -45,
//     51, -52, 43,  -55, -48, 54,  47,  -49, -43, 51,  52,  45,  -48, -44, 44,
//     45, -45, -46, -46, 55,  -49, 50,  57,  -44, -47, 47,  42,  38,  -50, -48,
//     49, 43,  -43, -50, -46, 48,  -57, -56, 48,  -48, 46,  -51, -47, 47,  38,
//     -52, -47, -46, -42, -48, -39, -46, -53, -44, 52,  -45, 48,  -55, -48, 37,
//     49, -33, -46, -48, -40, -50, -41, -51, -51, 51,  -43, 52,  44,  -60, -55,
//     -51, -41, 42,  -48, 46,  52,  -50, 45,  50,  -41, 40,  -55, 41,  52, -39,
//     -46, -46, -50, 55,  -46, 49,  48,  -44, 48,  40,  -38, -33, -40, -44,
//     -35, 44, 50,  45,  -39, -53, -44, -44, -44, 48,  -47, -48, -37, 57,  51,
//     51,  -51, 45,  44,  -36, 44,  43,  -47, 46,  42,  45,  40,  48,  -46,
//     -46, 49,  52, 60,  -49, -47, -52, -53, 47,  56,  -49, 51,  59,  47,  -46,
//     -40, -38, -45, 45,  -55, 33,  38,  -43, 43,  48,  49,  -42, 43,  -38, 43,
//     -48, -47, 42, -44, -51, 48,  -50, -54, 45,  42,  -46, 50,  55,  -43, 39,
//     -39, 38,  -39, 46,  45,  -36, -43, 48,  45,  59,  -45, -45, 49,  46, -52,
//     -46, -57, 50, -36, -43, 39,  -48, 45,  -45, -52, 54,  44,  -46, -51, -54,
//     -43, -50, -54, -47, -41, 53,  -51, 48,  41,  -58, -44, -38, -43, 43, -43,
//     52,  46,  -51, 45,  42,  -49, -35, -45, -45, -40, 58,  45,  54,  -41,
//     -52, -53, -51, -52, 44,  -42, -46, -44, 50,  46,  50,  -54, 58,  46, -50,
//     54,  39,  -48, 47, 46,  52,  51,  41,  -44, -46, 47,  48,  55,  -49, -53,
//     -52, -51, 51,  50, -54, 51,  47,  52,  -44, -46, -52, -45, 51,  -49, 48,
//     52,  -51, 45,  56, 57,  -46, 48,  -49, 41,  -51, -47, 49,  -51, -47, 38,
//     -55, -45, 51,  37, -42, 37,  40,  -42, 45,  -42, 48,  -38, 41,  47,  -47,
//     -46, 44,  50,  42, -46, -48, 47,  47,  -45, -44, -51, 45,  -46, -51, 52,
//     -44, 51,  -33, -51, 54,  53,  -44, -45, -51, -51, -40, -46, -54, -51, 40,
//     -49, 43,  51,  -41, -43, -45, -44, 55,  -48, 45,  47,  -48, 51,  56, -51,
//     -54, -47, -50, -44, 48,  42,  47,  -52, -43, -48, -49, -52, 52,  -58,
//     -54, -52, -43, -43, -54, -39, 46,  45,  48,  -50, -50, -55, -47, -41, 46,
//     -43, -45, -46, 45,  52, 45,  -48, 53,  47,  -44, 52,  49,  -45, 47,  42,
//     48,  47,  47,  -49, 50, 43,  54,  -58, 49,  49,  -48, 54,  57,  -48, 48,
//     54,  45,  61,  55,  -54, -48, 48,  54,  43,  -48, -52, -54, -42, 56,  46,
//     -51, 38,  58,  50,  -50, -56, -54, 49,  48,  58,  -48, -47, -42, -44, 53,
//     41,  -49, 48,  44,  47, -55, -47, 47,  -39, -41, 45,  -46, -47, 54,  47,
//     -48, 40,  46,  -40, 42, -45, 44,  -49, 39,  -40, -48, 51,  -43, -43, 56,
//     47,  -64, 43,  53,  -37, 50,  -53, 50,  -44, -47, -52, 54,  -40, 51,  42,
//     -50, 53,  44,  48,  -50, 51,  -41, 51,  -48, -48, -50, -41, 48,  -45, 50,
//     50,  -45, 31,  44,  46, -60, 38,  -49, 53,  -50, -45, 50,  45,  -46, -47,
//     43,  46,  54,  -56, -50, 45,  45,  -53, -48, -51, 42,  -48, 54,  37, -51,
//     -45, 55,  53,  45,  -47, -39, 55,  54,  -48, -50, -58, 42,  -47, -42, 45,
//     -44, 47,  -50, -49, 49, 43,  -49, -51, -44, -41, -47, -47, -44, -40, -45,
//     52,  -40, 51,  -46, -38, 39,  45,  -40, -57, -39, -39, -43, -37, -39};
// short PBCH_Info_index[56] = {
//     247, 253, 254, 255, 367, 375, 379, 381, 382, 383, 415, 431, 439, 441,
//     443, 444, 445, 446, 447, 463, 469, 470, 471, 473, 474, 475, 476, 477,
//     478, 479, 483, 485, 486, 487, 489, 490, 491, 492, 493, 494, 495, 497,
//     498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511};
// short iIL_56_PBCH[56] = {
//     0,  2,  3,  5,  7,  10, 11, 12, 14, 15, 18, 19, 21, 24, 26, 30, 31, 32,
//     1, 4,  6,  8,  13, 16, 20, 22, 25, 27, 33, 9,  17, 23, 28, 34, 29, 35,
//     36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
//     54, 55};

// // forzen bits R_Matrix
// short R_Matrix_0[512] = {
//     127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127,
//     127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127,
//     127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127,
//     127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127,
//     127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127,
//     127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127,
//     127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127,
//     127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127,
//     127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127,
//     127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127,
//     127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127,
//     127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127,
//     127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127,
//     127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127,
//     127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127,
//     127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127,
//     127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127,
//     127, 127, 127, 127, 127, 127, 127, 127, 127, 0,   127, 127, 127, 127,
//     127, 0,   0, 0,   127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127,
//     127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127,
//     127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127,
//     127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127,
//     127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127,
//     127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127,
//     127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127,
//     127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127,
//     127, 127, 0,   127, 127, 127, 127, 127, 127, 127, 0,   127, 127, 127, 0,
//     127, 0,   0,   0,   127, 127, 127, 127, 127, 127, 127, 127, 127, 127,
//     127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127,
//     127, 127, 127, 127, 127, 127, 127, 0,   127, 127, 127, 127, 127, 127,
//     127, 127, 127, 127, 127, 127, 127, 127, 127, 0,   127, 127, 127, 127,
//     127, 127, 127, 0,   127, 0,   127, 0,   0,   0,   0,   0,   127, 127,
//     127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 0, 127,
//     127, 127, 127, 127, 0,   0,   0,   127, 0,   0,   0,   0,   0,   0,   0,
//     127, 127, 127, 0,   127, 0,   0,   0,   127, 0,   0,   0,   0,   0,   0,
//     0,   127, 0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
//     0,   0};

// shuffle index
// short Xor_index[512] = {0, 0, 1, 0, 2, 0, 3, 0, 4, 0, 5, 0, 6, 0, 7, 0, 8, 0,
// 9, 0, 10, 0, 11, 0, 12, 0, 13, 0, 14, 0, 15, 0, 16, 0, 17, 0, 18, 0, 19, 0,
// 20, 0, 21, 0, 22, 0, 23, 0, 24, 0, 25, 0, 26, 0, 27, 0, 28, 0, 29, 0, 30, 0,
// 31, 0, 32, 0, 33, 0, 34, 0, 35, 0, 36, 0, 37, 0, 38, 0, 39, 0, 40, 0, 41, 0,
// 42, 0, 43, 0, 44, 0, 45, 0, 46, 0, 47, 0, 48, 0, 49, 0, 50, 0, 51, 0, 52, 0,
// 53, 0, 54, 0, 55, 0, 56, 0, 57, 0, 58, 0, 59, 0, 60, 0, 61, 0, 62, 0, 63, 0,
// 64, 0, 65, 0, 66, 0, 67, 0, 68, 0, 69, 0, 70, 0, 71, 0, 72, 0, 73, 0, 74, 0,
// 75, 0, 76, 0, 77, 0, 78, 0, 79, 0, 80, 0, 81, 0, 82, 0, 83, 0, 84, 0, 85, 0,
// 86, 0, 87, 0, 88, 0, 89, 0, 90, 0, 91, 0, 92, 0, 93, 0, 94, 0, 95, 0, 96, 0,
// 97, 0, 98, 0, 99, 0, 100, 0, 101, 0, 102, 0, 103, 0, 104, 0, 105, 0, 106, 0,
// 107, 0, 108, 0, 109, 0, 110, 0, 111, 0, 112, 0, 113, 0, 114, 0, 115, 0, 116,
// 0, 117, 0, 118, 0, 119, 0, 120, 0, 121, 0, 122, 0, 123, 0, 124, 0, 125, 0,
// 126, 0, 127, 0, 128, 0, 129, 0, 130, 0, 131, 0, 132, 0, 133, 0, 134, 0, 135,
// 0, 136, 0, 137, 0, 138, 0, 139, 0, 140, 0, 141, 0, 142, 0, 143, 0, 144, 0,
// 145, 0, 146, 0, 147, 0, 148, 0, 149, 0, 150, 0, 151, 0, 152, 0, 153, 0, 154,
// 0, 155, 0, 156, 0, 157, 0, 158, 0, 159, 0, 160, 0, 161, 0, 162, 0, 163, 0,
// 164, 0, 165, 0, 166, 0, 167, 0, 168, 0, 169, 0, 170, 0, 171, 0, 172, 0, 173,
// 0, 174, 0, 175, 0, 176, 0, 177, 0, 178, 0, 179, 0, 180, 0, 181, 0, 182, 0,
// 183, 0, 184, 0, 185, 0, 186, 0, 187, 0, 188, 0, 189, 0, 190, 0, 191, 0, 192,
// 0, 193, 0, 194, 0, 195, 0, 196, 0, 197, 0, 198, 0, 199, 0, 200, 0, 201, 0,
// 202, 0, 203, 0, 204, 0, 205, 0, 206, 0, 207, 0, 208, 0, 209, 0, 210, 0, 211,
// 0, 212, 0, 213, 0, 214, 0, 215, 0, 216, 0, 217, 0, 218, 0, 219, 0, 220, 0,
// 221, 0, 222, 0, 223, 0, 224, 0, 225, 0, 226, 0, 227, 0, 228, 0, 229, 0, 230,
// 0, 231, 0, 232, 0, 233, 0, 234, 0, 235, 0, 236, 0, 237, 0, 238, 0, 239, 0,
// 240, 0, 241, 0, 242, 0, 243, 0, 244, 0, 245, 0, 246, 0, 247, 0, 248, 0, 249,
// 0, 250, 0, 251, 0, 252, 0, 253, 0, 254, 0, 255, 0}; short Equal_index[512] =
// {0, 0, 0, 1, 0, 2, 0, 3, 0, 4, 0, 5, 0, 6, 0, 7, 0, 8, 0, 9, 0, 10, 0, 11, 0,
// 12, 0, 13, 0, 14, 0, 15, 0, 16, 0, 17, 0, 18, 0, 19, 0, 20, 0, 21, 0, 22, 0,
// 23, 0, 24, 0, 25, 0, 26, 0, 27, 0, 28, 0, 29, 0, 30, 0, 31, 0, 32, 0, 33, 0,
// 34, 0, 35, 0, 36, 0, 37, 0, 38, 0, 39, 0, 40, 0, 41, 0, 42, 0, 43, 0, 44, 0,
// 45, 0, 46, 0, 47, 0, 48, 0, 49, 0, 50, 0, 51, 0, 52, 0, 53, 0, 54, 0, 55, 0,
// 56, 0, 57, 0, 58, 0, 59, 0, 60, 0, 61, 0, 62, 0, 63, 0, 64, 0, 65, 0, 66, 0,
// 67, 0, 68, 0, 69, 0, 70, 0, 71, 0, 72, 0, 73, 0, 74, 0, 75, 0, 76, 0, 77, 0,
// 78, 0, 79, 0, 80, 0, 81, 0, 82, 0, 83, 0, 84, 0, 85, 0, 86, 0, 87, 0, 88, 0,
// 89, 0, 90, 0, 91, 0, 92, 0, 93, 0, 94, 0, 95, 0, 96, 0, 97, 0, 98, 0, 99, 0,
// 100, 0, 101, 0, 102, 0, 103, 0, 104, 0, 105, 0, 106, 0, 107, 0, 108, 0, 109,
// 0, 110, 0, 111, 0, 112, 0, 113, 0, 114, 0, 115, 0, 116, 0, 117, 0, 118, 0,
// 119, 0, 120, 0, 121, 0, 122, 0, 123, 0, 124, 0, 125, 0, 126, 0, 127, 0, 128,
// 0, 129, 0, 130, 0, 131, 0, 132, 0, 133, 0, 134, 0, 135, 0, 136, 0, 137, 0,
// 138, 0, 139, 0, 140, 0, 141, 0, 142, 0, 143, 0, 144, 0, 145, 0, 146, 0, 147,
// 0, 148, 0, 149, 0, 150, 0, 151, 0, 152, 0, 153, 0, 154, 0, 155, 0, 156, 0,
// 157, 0, 158, 0, 159, 0, 160, 0, 161, 0, 162, 0, 163, 0, 164, 0, 165, 0, 166,
// 0, 167, 0, 168, 0, 169, 0, 170, 0, 171, 0, 172, 0, 173, 0, 174, 0, 175, 0,
// 176, 0, 177, 0, 178, 0, 179, 0, 180, 0, 181, 0, 182, 0, 183, 0, 184, 0, 185,
// 0, 186, 0, 187, 0, 188, 0, 189, 0, 190, 0, 191, 0, 192, 0, 193, 0, 194, 0,
// 195, 0, 196, 0, 197, 0, 198, 0, 199, 0, 200, 0, 201, 0, 202, 0, 203, 0, 204,
// 0, 205, 0, 206, 0, 207, 0, 208, 0, 209, 0, 210, 0, 211, 0, 212, 0, 213, 0,
// 214, 0, 215, 0, 216, 0, 217, 0, 218, 0, 219, 0, 220, 0, 221, 0, 222, 0, 223,
// 0, 224, 0, 225, 0, 226, 0, 227, 0, 228, 0, 229, 0, 230, 0, 231, 0, 232, 0,
// 233, 0, 234, 0, 235, 0, 236, 0, 237, 0, 238, 0, 239, 0, 240, 0, 241, 0, 242,
// 0, 243, 0, 244, 0, 245, 0, 246, 0, 247, 0, 248, 0, 249, 0, 250, 0, 251, 0,
// 252, 0, 253, 0, 254, 0, 255};
// short Xor_up_index[256] = {
//     0,   0, 1,   0, 2,   0, 3,   0, 4,   0, 5,   0, 6,   0, 7,   0, 8,   0,
//     9,   0, 10,  0, 11,  0, 12,  0, 13,  0, 14,  0, 15,  0, 16,  0, 17,  0,
//     18,  0, 19,  0, 20,  0, 21,  0, 22,  0, 23,  0, 24,  0, 25,  0, 26,  0,
//     27,  0, 28,  0, 29,  0, 30,  0, 31,  0, 32,  0, 33,  0, 34,  0, 35,  0,
//     36,  0, 37,  0, 38,  0, 39,  0, 40,  0, 41,  0, 42,  0, 43,  0, 44,  0,
//     45,  0, 46,  0, 47,  0, 48,  0, 49,  0, 50,  0, 51,  0, 52,  0, 53,  0,
//     54,  0, 55,  0, 56,  0, 57,  0, 58,  0, 59,  0, 60,  0, 61,  0, 62,  0,
//     63,  0, 64,  0, 65,  0, 66,  0, 67,  0, 68,  0, 69,  0, 70,  0, 71,  0,
//     72,  0, 73,  0, 74,  0, 75,  0, 76,  0, 77,  0, 78,  0, 79,  0, 80,  0,
//     81,  0, 82,  0, 83,  0, 84,  0, 85,  0, 86,  0, 87,  0, 88,  0, 89,  0,
//     90,  0, 91,  0, 92,  0, 93,  0, 94,  0, 95,  0, 96,  0, 97,  0, 98,  0,
//     99,  0, 100, 0, 101, 0, 102, 0, 103, 0, 104, 0, 105, 0, 106, 0, 107, 0,
//     108, 0, 109, 0, 110, 0, 111, 0, 112, 0, 113, 0, 114, 0, 115, 0, 116, 0,
//     117, 0, 118, 0, 119, 0, 120, 0, 121, 0, 122, 0, 123, 0, 124, 0, 125, 0,
//     126, 0, 127, 0};
// short Xor_down_index[256] = {
//     128, 0, 129, 0, 130, 0, 131, 0, 132, 0, 133, 0, 134, 0, 135, 0, 136, 0,
//     137, 0, 138, 0, 139, 0, 140, 0, 141, 0, 142, 0, 143, 0, 144, 0, 145, 0,
//     146, 0, 147, 0, 148, 0, 149, 0, 150, 0, 151, 0, 152, 0, 153, 0, 154, 0,
//     155, 0, 156, 0, 157, 0, 158, 0, 159, 0, 160, 0, 161, 0, 162, 0, 163, 0,
//     164, 0, 165, 0, 166, 0, 167, 0, 168, 0, 169, 0, 170, 0, 171, 0, 172, 0,
//     173, 0, 174, 0, 175, 0, 176, 0, 177, 0, 178, 0, 179, 0, 180, 0, 181, 0,
//     182, 0, 183, 0, 184, 0, 185, 0, 186, 0, 187, 0, 188, 0, 189, 0, 190, 0,
//     191, 0, 192, 0, 193, 0, 194, 0, 195, 0, 196, 0, 197, 0, 198, 0, 199, 0,
//     200, 0, 201, 0, 202, 0, 203, 0, 204, 0, 205, 0, 206, 0, 207, 0, 208, 0,
//     209, 0, 210, 0, 211, 0, 212, 0, 213, 0, 214, 0, 215, 0, 216, 0, 217, 0,
//     218, 0, 219, 0, 220, 0, 221, 0, 222, 0, 223, 0, 224, 0, 225, 0, 226, 0,
//     227, 0, 228, 0, 229, 0, 230, 0, 231, 0, 232, 0, 233, 0, 234, 0, 235, 0,
//     236, 0, 237, 0, 238, 0, 239, 0, 240, 0, 241, 0, 242, 0, 243, 0, 244, 0,
//     245, 0, 246, 0, 247, 0, 248, 0, 249, 0, 250, 0, 251, 0, 252, 0, 253, 0,
//     254, 0, 255, 0};
// short Equal_up_index[256] = {
//     0, 0,   0, 1,   0, 2,   0, 3,   0, 4,   0, 5,   0, 6,   0, 7,   0, 8,
//     0, 9,   0, 10,  0, 11,  0, 12,  0, 13,  0, 14,  0, 15,  0, 16,  0, 17,
//     0, 18,  0, 19,  0, 20,  0, 21,  0, 22,  0, 23,  0, 24,  0, 25,  0, 26,
//     0, 27,  0, 28,  0, 29,  0, 30,  0, 31,  0, 32,  0, 33,  0, 34,  0, 35,
//     0, 36,  0, 37,  0, 38,  0, 39,  0, 40,  0, 41,  0, 42,  0, 43,  0, 44,
//     0, 45,  0, 46,  0, 47,  0, 48,  0, 49,  0, 50,  0, 51,  0, 52,  0, 53,
//     0, 54,  0, 55,  0, 56,  0, 57,  0, 58,  0, 59,  0, 60,  0, 61,  0, 62,
//     0, 63,  0, 64,  0, 65,  0, 66,  0, 67,  0, 68,  0, 69,  0, 70,  0, 71,
//     0, 72,  0, 73,  0, 74,  0, 75,  0, 76,  0, 77,  0, 78,  0, 79,  0, 80,
//     0, 81,  0, 82,  0, 83,  0, 84,  0, 85,  0, 86,  0, 87,  0, 88,  0, 89,
//     0, 90,  0, 91,  0, 92,  0, 93,  0, 94,  0, 95,  0, 96,  0, 97,  0, 98,
//     0, 99,  0, 100, 0, 101, 0, 102, 0, 103, 0, 104, 0, 105, 0, 106, 0, 107,
//     0, 108, 0, 109, 0, 110, 0, 111, 0, 112, 0, 113, 0, 114, 0, 115, 0, 116,
//     0, 117, 0, 118, 0, 119, 0, 120, 0, 121, 0, 122, 0, 123, 0, 124, 0, 125,
//     0, 126, 0, 127};
// short Equal_down_index[256] = {
//     0, 128, 0, 129, 0, 130, 0, 131, 0, 132, 0, 133, 0, 134, 0, 135, 0, 136,
//     0, 137, 0, 138, 0, 139, 0, 140, 0, 141, 0, 142, 0, 143, 0, 144, 0, 145,
//     0, 146, 0, 147, 0, 148, 0, 149, 0, 150, 0, 151, 0, 152, 0, 153, 0, 154,
//     0, 155, 0, 156, 0, 157, 0, 158, 0, 159, 0, 160, 0, 161, 0, 162, 0, 163,
//     0, 164, 0, 165, 0, 166, 0, 167, 0, 168, 0, 169, 0, 170, 0, 171, 0, 172,
//     0, 173, 0, 174, 0, 175, 0, 176, 0, 177, 0, 178, 0, 179, 0, 180, 0, 181,
//     0, 182, 0, 183, 0, 184, 0, 185, 0, 186, 0, 187, 0, 188, 0, 189, 0, 190,
//     0, 191, 0, 192, 0, 193, 0, 194, 0, 195, 0, 196, 0, 197, 0, 198, 0, 199,
//     0, 200, 0, 201, 0, 202, 0, 203, 0, 204, 0, 205, 0, 206, 0, 207, 0, 208,
//     0, 209, 0, 210, 0, 211, 0, 212, 0, 213, 0, 214, 0, 215, 0, 216, 0, 217,
//     0, 218, 0, 219, 0, 220, 0, 221, 0, 222, 0, 223, 0, 224, 0, 225, 0, 226,
//     0, 227, 0, 228, 0, 229, 0, 230, 0, 231, 0, 232, 0, 233, 0, 234, 0, 235,
//     0, 236, 0, 237, 0, 238, 0, 239, 0, 240, 0, 241, 0, 242, 0, 243, 0, 244,
//     0, 245, 0, 246, 0, 247, 0, 248, 0, 249, 0, 250, 0, 251, 0, 252, 0, 253,
//     0, 254, 0, 255};

// short index_front_024[256] = {
//     0,   2,   4,   6,   8,   10,  12,  14,  16,  18,  20,  22,  24,  26,  28,
//     30,  32,  34,  36,  38,  40,  42,  44,  46,  48,  50,  52,  54,  56,  58,
//     60,  62,  64,  66,  68,  70,  72,  74,  76,  78,  80,  82,  84,  86,  88,
//     90,  92,  94,  96,  98,  100, 102, 104, 106, 108, 110, 112, 114, 116,
//     118, 120, 122, 124, 126, 128, 130, 132, 134, 136, 138, 140, 142, 144,
//     146, 148, 150, 152, 154, 156, 158, 160, 162, 164, 166, 168, 170, 172,
//     174, 176, 178, 180, 182, 184, 186, 188, 190, 192, 194, 196, 198, 200,
//     202, 204, 206, 208, 210, 212, 214, 216, 218, 220, 222, 224, 226, 228,
//     230, 232, 234, 236, 238, 240, 242, 244, 246, 248, 250, 252, 254, 0,   0,
//     0,   0,   0,   0,   0, 0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 0,
//     0,   0,   0,   0, 0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 0,
//     0,   0,   0, 0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 0,
//     0,   0, 0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 0,
//     0, 0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 0,
//     0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
//     0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
//     0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
//     0};
// short index_back_024[256] = {
//     0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
//     0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
//     0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
//     0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
//     0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
//     0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
//     0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
//     0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
//     0,   0,   0,   0,   0,   0,   0,   0,   0,   2,   4,   6,   8,   10,  12,
//     14,  16,  18,  20,  22,  24,  26,  28,  30,  32,  34,  36,  38,  40,  42,
//     44,  46,  48,  50,  52,  54,  56,  58,  60,  62,  64,  66,  68,  70,  72,
//     74,  76,  78,  80,  82,  84,  86,  88,  90,  92,  94,  96,  98,  100,
//     102, 104, 106, 108, 110, 112, 114, 116, 118, 120, 122, 124, 126, 128,
//     130, 132, 134, 136, 138, 140, 142, 144, 146, 148, 150, 152, 154, 156,
//     158, 160, 162, 164, 166, 168, 170, 172, 174, 176, 178, 180, 182, 184,
//     186, 188, 190, 192, 194, 196, 198, 200, 202, 204, 206, 208, 210, 212,
//     214, 216, 218, 220, 222, 224, 226, 228, 230, 232, 234, 236, 238, 240,
//     242, 244, 246, 248, 250, 252, 254};
// short index_front_135[256] = {
//     1,   3,   5,   7,   9,   11,  13,  15,  17,  19,  21,  23,  25,  27,  29,
//     31,  33,  35,  37,  39,  41,  43,  45,  47,  49,  51,  53,  55,  57,  59,
//     61,  63,  65,  67,  69,  71,  73,  75,  77,  79,  81,  83,  85,  87,  89,
//     91,  93,  95,  97,  99,  101, 103, 105, 107, 109, 111, 113, 115, 117,
//     119, 121, 123, 125, 127, 129, 131, 133, 135, 137, 139, 141, 143, 145,
//     147, 149, 151, 153, 155, 157, 159, 161, 163, 165, 167, 169, 171, 173,
//     175, 177, 179, 181, 183, 185, 187, 189, 191, 193, 195, 197, 199, 201,
//     203, 205, 207, 209, 211, 213, 215, 217, 219, 221, 223, 225, 227, 229,
//     231, 233, 235, 237, 239, 241, 243, 245, 247, 249, 251, 253, 255, 0,   0,
//     0,   0,   0,   0,   0, 0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 0,
//     0,   0,   0,   0, 0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 0,
//     0,   0,   0, 0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 0,
//     0,   0, 0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 0,
//     0, 0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 0,
//     0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
//     0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
//     0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
//     0};
// short index_back_135[256] = {
//     0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
//     0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
//     0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
//     0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
//     0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
//     0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
//     0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
//     0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
//     0,   0,   0,   0,   0,   0,   0,   0,   1,   3,   5,   7,   9,   11,  13,
//     15,  17,  19,  21,  23,  25,  27,  29,  31,  33,  35,  37,  39,  41,  43,
//     45,  47,  49,  51,  53,  55,  57,  59,  61,  63,  65,  67,  69,  71,  73,
//     75,  77,  79,  81,  83,  85,  87,  89,  91,  93,  95,  97,  99,  101,
//     103, 105, 107, 109, 111, 113, 115, 117, 119, 121, 123, 125, 127, 129,
//     131, 133, 135, 137, 139, 141, 143, 145, 147, 149, 151, 153, 155, 157,
//     159, 161, 163, 165, 167, 169, 171, 173, 175, 177, 179, 181, 183, 185,
//     187, 189, 191, 193, 195, 197, 199, 201, 203, 205, 207, 209, 211, 213,
//     215, 217, 219, 221, 223, 225, 227, 229, 231, 233, 235, 237, 239, 241,
//     243, 245, 247, 249, 251, 253, 255};

// short Mask_1010_table[256] = {
//     1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
//     1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
//     1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
//     1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
//     1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
//     1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
//     1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
//     1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
//     1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
//     1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
//     1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0};
// short Mask_0101_table[256] = {
//     0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
//     0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
//     0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
//     0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
//     0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
//     0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
//     0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
//     0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
//     0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
//     0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
//     0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1};

//---------------------------------------------------------------//

// 输入定点化的点数，实现定点乘法的输出
VENUS_INLINE __v4096i8 MUL4096_8_FIXED(__v4096i8 a, __v4096i8 b, int fix_point, int length) {
  __v4096i8 high8;
  __v4096i8 low8;
  __v4096i8 result;
  short     high_shift = 8 - fix_point;

  low8   = vmul(a, b, MASKREAD_OFF, length);
  high8  = vmulh(a, b, MASKREAD_OFF, length);
  low8   = vsrl(low8, fix_point, MASKREAD_OFF, length);
  high8  = vsll(high8, high_shift, MASKREAD_OFF, length);
  result = vor(low8, high8, MASKREAD_OFF, length);
  return result;
};

VENUS_INLINE __v4096i8 raterecover_polar_downlink(__v4096i8 vin, uint16_t E, uint16_t K, uint16_t N, uint16_t IBIL,
                                                  __v2048i16 pi) {
  __v4096i8  vout;
  __v2048i16 index;
  // __v2048i16 pi;
  __v2048i16 jn;
  __v2048i16 ii;
  __v2048i16 n;

  vclaim(vout);
  vclaim(index);
  // vclaim(pi);
  vclaim(jn);
  vclaim(ii);
  vclaim(n);

  uint16_t N_32    = (N >> 5);
  uint16_t logN_32 = 0;

  for (int i = N_32; i > 1; i >>= 1)
    logN_32++;

  // int pi_addr = vaddr(pi);
  // vbarrier();
  // VSPM_OPEN();
  // for (size_t i = 0; i < 32; i++) {
  //   unsigned int addr = pi_addr + (i << 1);
  //   *(volatile unsigned short *)(addr) = pi_[i];
  // }
  // VSPM_CLOSE();

  if (E >= N) {
    vout = vsadd(vin, 0, MASKREAD_OFF, N);
  } else {
    if ((K * 64 / E) <= 28) {
      vrange(index, E);
      index = vsadd(index, N - E, MASKREAD_OFF, E);
      vshuffle(vout, index, vin, SHUFFLE_SCATTER, E);
    } else {
      vbrdcst(vout, INF, MASKREAD_OFF, N);
      vout = vsadd(vin, 0, MASKREAD_OFF, E);
    }
  }

  vbrdcst(jn, 0, MASKREAD_OFF, N);

  vrange(ii, N);
  ii = vsll(ii, 5, MASKREAD_OFF, N);
  ii = vsrl(ii, 9, MASKREAD_OFF, N);

  vshuffle(jn, ii, pi, SHUFFLE_GATHER, N);
  jn = vmul(jn, N_32, MASKREAD_OFF, N);

  vrange(ii, N);
  ii = vsrl(ii, logN_32, MASKREAD_OFF, N);
  ii = vsll(ii, logN_32, MASKREAD_OFF, N);
  vrange(n, N);
  n  = vrsub(n, ii, MASKREAD_OFF, N);
  jn = vsadd(jn, n, MASKREAD_OFF, N);

  vshuffle(vout, jn, vout, SHUFFLE_SCATTER, N);

  return vout;
}

VENUS_INLINE __v4096i8 polar_512bits(__v4096i8 vin, uint16_t K, uint16_t E, uint16_t RNTI, uint16_t iternumber,
                                     __v2048i16 Xor_index_up, __v2048i16 Xor_index_down, __v2048i16 Equal_index_up,
                                     __v2048i16 Equal_index_down, __v4096i8 Mask_1010, __v4096i8 Mask_0101,
                                     __v2048i16 index_front_odd, __v2048i16 index_back_odd, __v2048i16 index_front_even,
                                     __v2048i16 index_back_even, __v4096i8 R_Matrix_0_up, __v4096i8 R_Matrix_0_down,
                                     __v2048i16 PBCH_bit_Index, __v2048i16 iIL_index) {
  short Polar_Length = 512;
  // __v2048i16 Xor_index_up;
  // __v2048i16 Xor_index_down;
  // __v2048i16 Equal_index_up;
  // __v2048i16 Equal_index_down;
  // vclaim(Xor_index_up);
  // vclaim(Xor_index_down);
  // vclaim(Equal_index_up);
  // vclaim(Equal_index_down);

  // vbarrier();
  // VSPM_OPEN();
  // int Xor_index_up_addr = vaddr(Xor_index_up);
  // for (int i = 0; i < 256; i++) {
  //   *(volatile unsigned short *)(Xor_index_up_addr + (i << 1)) =
  //       Xor_up_index[i];
  // }
  // int Xor_index_down_addr = vaddr(Xor_index_down);
  // for (int i = 0; i < 256; i++) {
  //   *(volatile unsigned short *)(Xor_index_down_addr + (i << 1)) =
  //       Xor_down_index[i];
  // }
  // int Equal_index_up_addr = vaddr(Equal_index_up);
  // for (int i = 0; i < 256; i++) {
  //   *(volatile unsigned short *)(Equal_index_up_addr + (i << 1)) =
  //       Equal_up_index[i];
  // }
  // int Equal_index_down_addr = vaddr(Equal_index_down);
  // for (int i = 0; i < 256; i++) {
  //   *(volatile unsigned short *)(Equal_index_down_addr + (i << 1)) =
  //       Equal_down_index[i];
  // }
  // VSPM_CLOSE();

  // __v4096i8 Mask_1010;
  // __v4096i8 Mask_0101;
  // vclaim(Mask_1010);
  // vclaim(Mask_0101);

  // vbarrier();
  // VSPM_OPEN();
  // int Mask_1010_addr = vaddr(Mask_1010);
  // for (int i = 0; i < 256; i++) {
  //   *(volatile unsigned char *)(Mask_1010_addr + i) = Mask_1010_table[i];
  // }
  // int Mask_0101_addr = vaddr(Mask_0101);
  // for (int i = 0; i < 256; i++) {
  //   *(volatile unsigned char *)(Mask_0101_addr + i) = Mask_0101_table[i];
  // }
  // VSPM_CLOSE();

  // __v2048i16 index_front_odd;
  // __v2048i16 index_back_odd;
  // __v2048i16 index_front_even;
  // __v2048i16 index_back_even;
  // vclaim(index_front_odd);
  // vclaim(index_back_odd);
  // vclaim(index_front_even);
  // vclaim(index_back_even);

  // vbarrier();
  // VSPM_OPEN();
  // int index_front_odd_addr = vaddr(index_front_odd);
  // for (int i = 0; i < 256; i++) {
  //   *(volatile unsigned short *)(index_front_odd_addr + (i << 1)) =
  //       index_front_135[i];
  // }
  // int index_back_odd_addr = vaddr(index_back_odd);
  // for (int i = 0; i < 256; i++) {
  //   *(volatile unsigned short *)(index_back_odd_addr + (i << 1)) =
  //       index_back_135[i];
  // }
  // int index_front_even_addr = vaddr(index_front_even);
  // for (int i = 0; i < 256; i++) {
  //   *(volatile unsigned short *)(index_front_even_addr + (i << 1)) =
  //       index_front_024[i];
  // }
  // int index_back_even_addr = vaddr(index_back_even);
  // for (int i = 0; i < 256; i++) {
  //   *(volatile unsigned short *)(index_back_even_addr + (i << 1)) =
  //       index_back_024[i];
  // }
  // VSPM_CLOSE();

  __v4096i8 Mask_000111;
  __v4096i8 Mask_111000;
  vclaim(Mask_000111);
  vclaim(Mask_111000);
  vbrdcst(Mask_000111, 1, MASKREAD_OFF, 256);
  vbrdcst(Mask_000111, 0, MASKREAD_OFF, 128);
  Mask_000111 = vsadd(Mask_000111, 0, MASKREAD_OFF, 256); // 冗余
  vbrdcst(Mask_111000, 0, MASKREAD_OFF, 256);
  vbrdcst(Mask_111000, 1, MASKREAD_OFF, 128);
  Mask_111000 = vsadd(Mask_111000, 0, MASKREAD_OFF, 256); // 冗余

  __v4096i8 L_Matrix_N_up;
  __v4096i8 L_Matrix_N_down;
  vclaim(L_Matrix_N_up);
  vclaim(L_Matrix_N_down);

  // VSPM_OPEN();
  // vbarrier();
  // int L_Matrix_N_up_addr = vaddr(L_Matrix_N_up);
  // for (int i = 0; i < 256; i++)
  // {
  //     *(volatile unsigned char *)(L_Matrix_N_up_addr + i) = input_llr[i];
  // }
  // int L_Matrix_N_down_addr = vaddr(L_Matrix_N_down);
  // for (int i = 0; i < 256; i++)
  // {
  //     *(volatile unsigned char *)(L_Matrix_N_down_addr + i) = input_llr[256 +
  //     i];
  // }
  // VSPM_CLOSE();
  __v2048i16 index_input;
  vrange(index_input, 256);
  vshuffle(L_Matrix_N_up, index_input, vin, SHUFFLE_GATHER, 256);
  index_input = vsadd(index_input, 256, MASKREAD_OFF, 256);
  vshuffle(L_Matrix_N_down, index_input, vin, SHUFFLE_GATHER, 256);

  // __v4096i8 R_Matrix_0_up;
  // __v4096i8 R_Matrix_0_down;
  // vclaim(R_Matrix_0_up);
  // vclaim(R_Matrix_0_down);

  // vbarrier();
  // VSPM_OPEN();
  // int R_Matrix_0_up_addr = vaddr(R_Matrix_0_up);
  // for (int i = 0; i < 256; i++) {
  //   *(volatile unsigned char *)(R_Matrix_0_up_addr + i) = R_Matrix_0[i];
  // }
  // int R_Matrix_0_down_addr = vaddr(R_Matrix_0_down);
  // for (int i = 0; i < 256; i++) {
  //   *(volatile unsigned char *)(R_Matrix_0_down_addr + i) = R_Matrix_0[256 +
  //   i];
  // }
  // VSPM_CLOSE();

  __v4096i8 alpha;
  vclaim(alpha);
  vbrdcst(alpha, 60, MASKREAD_OFF, 256);
  alpha = vsadd(alpha, 0, MASKREAD_OFF, 256); // 冗余

  // 生成R矩阵
  __v4096i8 R_Matrix_1_up;
  __v4096i8 R_Matrix_1_down;
  vclaim(R_Matrix_1_up);
  vclaim(R_Matrix_1_down);
  __v4096i8 R_Matrix_2_up;
  __v4096i8 R_Matrix_2_down;
  vclaim(R_Matrix_2_up);
  vclaim(R_Matrix_2_down);
  __v4096i8 R_Matrix_3_up;
  __v4096i8 R_Matrix_3_down;
  vclaim(R_Matrix_3_up);
  vclaim(R_Matrix_3_down);
  __v4096i8 R_Matrix_4_up;
  __v4096i8 R_Matrix_4_down;
  vclaim(R_Matrix_4_up);
  vclaim(R_Matrix_4_down);
  __v4096i8 R_Matrix_5_up;
  __v4096i8 R_Matrix_5_down;
  vclaim(R_Matrix_5_up);
  vclaim(R_Matrix_5_down);
  __v4096i8 R_Matrix_6_up;
  __v4096i8 R_Matrix_6_down;
  vclaim(R_Matrix_6_up);
  vclaim(R_Matrix_6_down);
  __v4096i8 R_Matrix_7_up;
  __v4096i8 R_Matrix_7_down;
  vclaim(R_Matrix_7_up);
  vclaim(R_Matrix_7_down);
  __v4096i8 R_Matrix_8_up;
  __v4096i8 R_Matrix_8_down;
  vclaim(R_Matrix_8_up);
  vclaim(R_Matrix_8_down);
  __v4096i8 R_Matrix_9_up;
  __v4096i8 R_Matrix_9_down;
  vclaim(R_Matrix_9_up);
  vclaim(R_Matrix_9_down);

  //  冗余
  R_Matrix_9_up   = vmul(alpha, 0, MASKREAD_OFF, 256);
  R_Matrix_9_down = vmul(alpha, 0, MASKREAD_OFF, 256);
  R_Matrix_8_up   = vmul(alpha, 0, MASKREAD_OFF, 256);
  R_Matrix_8_down = vmul(alpha, 0, MASKREAD_OFF, 256);
  R_Matrix_7_up   = vmul(alpha, 0, MASKREAD_OFF, 256);
  R_Matrix_7_down = vmul(alpha, 0, MASKREAD_OFF, 256);
  R_Matrix_6_up   = vmul(alpha, 0, MASKREAD_OFF, 256);
  R_Matrix_6_down = vmul(alpha, 0, MASKREAD_OFF, 256);
  R_Matrix_5_up   = vmul(alpha, 0, MASKREAD_OFF, 256);
  R_Matrix_5_down = vmul(alpha, 0, MASKREAD_OFF, 256);
  R_Matrix_4_up   = vmul(alpha, 0, MASKREAD_OFF, 256);
  R_Matrix_4_down = vmul(alpha, 0, MASKREAD_OFF, 256);
  R_Matrix_3_up   = vmul(alpha, 0, MASKREAD_OFF, 256);
  R_Matrix_3_down = vmul(alpha, 0, MASKREAD_OFF, 256);
  R_Matrix_2_up   = vmul(alpha, 0, MASKREAD_OFF, 256);
  R_Matrix_2_down = vmul(alpha, 0, MASKREAD_OFF, 256);
  R_Matrix_1_up   = vmul(alpha, 0, MASKREAD_OFF, 256);
  R_Matrix_1_down = vmul(alpha, 0, MASKREAD_OFF, 256);

  // 生成L矩阵
  __v4096i8 L_Matrix_0_up;
  __v4096i8 L_Matrix_0_down;
  vclaim(L_Matrix_0_up);
  vclaim(L_Matrix_0_down);
  __v4096i8 L_Matrix_1_up;
  __v4096i8 L_Matrix_1_down;
  vclaim(L_Matrix_1_up);
  vclaim(L_Matrix_1_down);
  __v4096i8 L_Matrix_2_up;
  __v4096i8 L_Matrix_2_down;
  vclaim(L_Matrix_2_up);
  vclaim(L_Matrix_2_down);
  __v4096i8 L_Matrix_3_up;
  __v4096i8 L_Matrix_3_down;
  vclaim(L_Matrix_3_up);
  vclaim(L_Matrix_3_down);
  __v4096i8 L_Matrix_4_up;
  __v4096i8 L_Matrix_4_down;
  vclaim(L_Matrix_4_up);
  vclaim(L_Matrix_4_down);
  __v4096i8 L_Matrix_5_up;
  __v4096i8 L_Matrix_5_down;
  vclaim(L_Matrix_5_up);
  vclaim(L_Matrix_5_down);
  __v4096i8 L_Matrix_6_up;
  __v4096i8 L_Matrix_6_down;
  vclaim(L_Matrix_6_up);
  vclaim(L_Matrix_6_down);
  __v4096i8 L_Matrix_7_up;
  __v4096i8 L_Matrix_7_down;
  vclaim(L_Matrix_7_up);
  vclaim(L_Matrix_7_down);
  __v4096i8 L_Matrix_8_up;
  __v4096i8 L_Matrix_8_down;
  vclaim(L_Matrix_8_up);
  vclaim(L_Matrix_8_down);

  //  冗余
  L_Matrix_8_up   = vmul(alpha, 0, MASKREAD_OFF, 256);
  L_Matrix_8_down = vmul(alpha, 0, MASKREAD_OFF, 256);
  L_Matrix_7_up   = vmul(alpha, 0, MASKREAD_OFF, 256);
  L_Matrix_7_down = vmul(alpha, 0, MASKREAD_OFF, 256);
  L_Matrix_6_up   = vmul(alpha, 0, MASKREAD_OFF, 256);
  L_Matrix_6_down = vmul(alpha, 0, MASKREAD_OFF, 256);
  L_Matrix_5_up   = vmul(alpha, 0, MASKREAD_OFF, 256);
  L_Matrix_5_down = vmul(alpha, 0, MASKREAD_OFF, 256);
  L_Matrix_4_up   = vmul(alpha, 0, MASKREAD_OFF, 256);
  L_Matrix_4_down = vmul(alpha, 0, MASKREAD_OFF, 256);
  L_Matrix_3_up   = vmul(alpha, 0, MASKREAD_OFF, 256);
  L_Matrix_3_down = vmul(alpha, 0, MASKREAD_OFF, 256);
  L_Matrix_2_up   = vmul(alpha, 0, MASKREAD_OFF, 256);
  L_Matrix_2_down = vmul(alpha, 0, MASKREAD_OFF, 256);
  L_Matrix_1_up   = vmul(alpha, 0, MASKREAD_OFF, 256);
  L_Matrix_1_down = vmul(alpha, 0, MASKREAD_OFF, 256);
  L_Matrix_0_up   = vmul(alpha, 0, MASKREAD_OFF, 256);
  L_Matrix_0_down = vmul(alpha, 0, MASKREAD_OFF, 256);

  __v2048i16 copy_index;
  vclaim(copy_index);
  vrange(copy_index, 256);

  // __v4096i8 b_add_d;
  // vclaim(b_add_d);

  __v4096i8 temp_a;
  __v4096i8 temp_b;
  __v4096i8 temp_c;
  __v4096i8 temp_d;
  vclaim(temp_a);
  vclaim(temp_b);
  vclaim(temp_c);
  vclaim(temp_d);
  __v4096i8 result_up_temp;
  vclaim(result_up_temp);
  __v4096i8 result_up_temp_a;
  vclaim(result_up_temp_a);
  __v4096i8 min_bd_c;
  vclaim(min_bd_c);
  __v4096i8 temp;
  vclaim(temp);
  __v4096i8 temp1;
  vclaim(temp1);
  //  迭代Polar译码
  for (int iter = 0; iter < iternumber; iter++) {
    //  L Matrix Calculation
    for (int column = 9; column > 0; column--) {
      __v4096i8 indata_up_a;
      __v4096i8 indata_up_b;
      __v4096i8 indata_down_a;
      __v4096i8 indata_down_b;
      indata_up_a   = vmul(alpha, 0, MASKREAD_OFF, 256);
      indata_up_b   = vmul(alpha, 0, MASKREAD_OFF, 256);
      indata_down_a = vmul(alpha, 0, MASKREAD_OFF, 256);
      indata_down_b = vmul(alpha, 0, MASKREAD_OFF, 256);

      if (column == 9) {
        vshuffle(indata_up_a, index_front_even, L_Matrix_N_up, SHUFFLE_GATHER,
                 256); // even = {0,2,4,6,...}
        vshuffle(indata_up_b, index_back_even, L_Matrix_N_down, SHUFFLE_GATHER, 256);
        vshuffle(indata_down_a, index_front_odd, L_Matrix_N_up, SHUFFLE_GATHER, 256);
        vshuffle(indata_down_b, index_back_odd, L_Matrix_N_down, SHUFFLE_GATHER, 256);
        indata_up_a   = vmul(indata_up_a, Mask_111000, MASKREAD_OFF, 256);
        indata_up_b   = vmul(indata_up_b, Mask_000111, MASKREAD_OFF, 256);
        indata_down_a = vmul(indata_down_a, Mask_111000, MASKREAD_OFF, 256);
        indata_down_b = vmul(indata_down_b, Mask_000111, MASKREAD_OFF, 256);

        temp_a = vsadd(indata_up_a, indata_up_b, MASKREAD_OFF, 256);
        temp_b = vsadd(indata_down_a, indata_down_b, MASKREAD_OFF, 256);
        vshuffle(temp_c, copy_index, R_Matrix_8_up, SHUFFLE_GATHER, 256);
        vshuffle(temp_d, copy_index, R_Matrix_8_down, SHUFFLE_GATHER, 256);
      } else if (column == 8) {
        vshuffle(indata_up_a, index_front_even, L_Matrix_8_up, SHUFFLE_GATHER,
                 256); // even = {0,2,4,6,...}
        vshuffle(indata_up_b, index_back_even, L_Matrix_8_down, SHUFFLE_GATHER, 256);
        vshuffle(indata_down_a, index_front_odd, L_Matrix_8_up, SHUFFLE_GATHER, 256);
        vshuffle(indata_down_b, index_back_odd, L_Matrix_8_down, SHUFFLE_GATHER, 256);
        indata_up_a   = vmul(indata_up_a, Mask_111000, MASKREAD_OFF, 256);
        indata_up_b   = vmul(indata_up_b, Mask_000111, MASKREAD_OFF, 256);
        indata_down_a = vmul(indata_down_a, Mask_111000, MASKREAD_OFF, 256);
        indata_down_b = vmul(indata_down_b, Mask_000111, MASKREAD_OFF, 256);

        temp_a = vsadd(indata_up_a, indata_up_b, MASKREAD_OFF, 256);
        temp_b = vsadd(indata_down_a, indata_down_b, MASKREAD_OFF, 256);
        vshuffle(temp_c, copy_index, R_Matrix_7_up, SHUFFLE_GATHER, 256);
        vshuffle(temp_d, copy_index, R_Matrix_7_down, SHUFFLE_GATHER, 256);
      } else if (column == 7) {
        vshuffle(indata_up_a, index_front_even, L_Matrix_7_up, SHUFFLE_GATHER,
                 256); // even = {0,2,4,6,...}
        vshuffle(indata_up_b, index_back_even, L_Matrix_7_down, SHUFFLE_GATHER, 256);
        vshuffle(indata_down_a, index_front_odd, L_Matrix_7_up, SHUFFLE_GATHER, 256);
        vshuffle(indata_down_b, index_back_odd, L_Matrix_7_down, SHUFFLE_GATHER, 256);
        indata_up_a   = vmul(indata_up_a, Mask_111000, MASKREAD_OFF, 256);
        indata_up_b   = vmul(indata_up_b, Mask_000111, MASKREAD_OFF, 256);
        indata_down_a = vmul(indata_down_a, Mask_111000, MASKREAD_OFF, 256);
        indata_down_b = vmul(indata_down_b, Mask_000111, MASKREAD_OFF, 256);

        temp_a = vsadd(indata_up_a, indata_up_b, MASKREAD_OFF, 256);
        temp_b = vsadd(indata_down_a, indata_down_b, MASKREAD_OFF, 256);
        vshuffle(temp_c, copy_index, R_Matrix_6_up, SHUFFLE_GATHER, 256);
        vshuffle(temp_d, copy_index, R_Matrix_6_down, SHUFFLE_GATHER, 256);
      } else if (column == 6) {
        vshuffle(indata_up_a, index_front_even, L_Matrix_6_up, SHUFFLE_GATHER,
                 256); // even = {0,2,4,6,...}
        vshuffle(indata_up_b, index_back_even, L_Matrix_6_down, SHUFFLE_GATHER, 256);
        vshuffle(indata_down_a, index_front_odd, L_Matrix_6_up, SHUFFLE_GATHER, 256);
        vshuffle(indata_down_b, index_back_odd, L_Matrix_6_down, SHUFFLE_GATHER, 256);
        indata_up_a   = vmul(indata_up_a, Mask_111000, MASKREAD_OFF, 256);
        indata_up_b   = vmul(indata_up_b, Mask_000111, MASKREAD_OFF, 256);
        indata_down_a = vmul(indata_down_a, Mask_111000, MASKREAD_OFF, 256);
        indata_down_b = vmul(indata_down_b, Mask_000111, MASKREAD_OFF, 256);

        temp_a = vsadd(indata_up_a, indata_up_b, MASKREAD_OFF, 256);
        temp_b = vsadd(indata_down_a, indata_down_b, MASKREAD_OFF, 256);
        vshuffle(temp_c, copy_index, R_Matrix_5_up, SHUFFLE_GATHER, 256);
        vshuffle(temp_d, copy_index, R_Matrix_5_down, SHUFFLE_GATHER, 256);
      } else if (column == 5) {
        vshuffle(indata_up_a, index_front_even, L_Matrix_5_up, SHUFFLE_GATHER,
                 256); // even = {0,2,4,6,...}
        vshuffle(indata_up_b, index_back_even, L_Matrix_5_down, SHUFFLE_GATHER, 256);
        vshuffle(indata_down_a, index_front_odd, L_Matrix_5_up, SHUFFLE_GATHER, 256);
        vshuffle(indata_down_b, index_back_odd, L_Matrix_5_down, SHUFFLE_GATHER, 256);
        indata_up_a   = vmul(indata_up_a, Mask_111000, MASKREAD_OFF, 256);
        indata_up_b   = vmul(indata_up_b, Mask_000111, MASKREAD_OFF, 256);
        indata_down_a = vmul(indata_down_a, Mask_111000, MASKREAD_OFF, 256);
        indata_down_b = vmul(indata_down_b, Mask_000111, MASKREAD_OFF, 256);

        temp_a = vsadd(indata_up_a, indata_up_b, MASKREAD_OFF, 256);
        temp_b = vsadd(indata_down_a, indata_down_b, MASKREAD_OFF, 256);
        vshuffle(temp_c, copy_index, R_Matrix_4_up, SHUFFLE_GATHER, 256);
        vshuffle(temp_d, copy_index, R_Matrix_4_down, SHUFFLE_GATHER, 256);
      } else if (column == 4) {
        vshuffle(indata_up_a, index_front_even, L_Matrix_4_up, SHUFFLE_GATHER,
                 256); // even = {0,2,4,6,...}
        vshuffle(indata_up_b, index_back_even, L_Matrix_4_down, SHUFFLE_GATHER, 256);
        vshuffle(indata_down_a, index_front_odd, L_Matrix_4_up, SHUFFLE_GATHER, 256);
        vshuffle(indata_down_b, index_back_odd, L_Matrix_4_down, SHUFFLE_GATHER, 256);
        indata_up_a   = vmul(indata_up_a, Mask_111000, MASKREAD_OFF, 256);
        indata_up_b   = vmul(indata_up_b, Mask_000111, MASKREAD_OFF, 256);
        indata_down_a = vmul(indata_down_a, Mask_111000, MASKREAD_OFF, 256);
        indata_down_b = vmul(indata_down_b, Mask_000111, MASKREAD_OFF, 256);

        temp_a = vsadd(indata_up_a, indata_up_b, MASKREAD_OFF, 256);
        temp_b = vsadd(indata_down_a, indata_down_b, MASKREAD_OFF, 256);
        vshuffle(temp_c, copy_index, R_Matrix_3_up, SHUFFLE_GATHER, 256);
        vshuffle(temp_d, copy_index, R_Matrix_3_down, SHUFFLE_GATHER, 256);
      } else if (column == 3) {
        vshuffle(indata_up_a, index_front_even, L_Matrix_3_up, SHUFFLE_GATHER,
                 256); // even = {0,2,4,6,...}
        vshuffle(indata_up_b, index_back_even, L_Matrix_3_down, SHUFFLE_GATHER, 256);
        vshuffle(indata_down_a, index_front_odd, L_Matrix_3_up, SHUFFLE_GATHER, 256);
        vshuffle(indata_down_b, index_back_odd, L_Matrix_3_down, SHUFFLE_GATHER, 256);
        indata_up_a   = vmul(indata_up_a, Mask_111000, MASKREAD_OFF, 256);
        indata_up_b   = vmul(indata_up_b, Mask_000111, MASKREAD_OFF, 256);
        indata_down_a = vmul(indata_down_a, Mask_111000, MASKREAD_OFF, 256);
        indata_down_b = vmul(indata_down_b, Mask_000111, MASKREAD_OFF, 256);

        temp_a = vsadd(indata_up_a, indata_up_b, MASKREAD_OFF, 256);
        temp_b = vsadd(indata_down_a, indata_down_b, MASKREAD_OFF, 256);
        vshuffle(temp_c, copy_index, R_Matrix_2_up, SHUFFLE_GATHER, 256);
        vshuffle(temp_d, copy_index, R_Matrix_2_down, SHUFFLE_GATHER, 256);
      } else if (column == 2) {
        vshuffle(indata_up_a, index_front_even, L_Matrix_2_up, SHUFFLE_GATHER,
                 256); // even = {0,2,4,6,...}
        vshuffle(indata_up_b, index_back_even, L_Matrix_2_down, SHUFFLE_GATHER, 256);
        vshuffle(indata_down_a, index_front_odd, L_Matrix_2_up, SHUFFLE_GATHER, 256);
        vshuffle(indata_down_b, index_back_odd, L_Matrix_2_down, SHUFFLE_GATHER, 256);
        indata_up_a   = vmul(indata_up_a, Mask_111000, MASKREAD_OFF, 256);
        indata_up_b   = vmul(indata_up_b, Mask_000111, MASKREAD_OFF, 256);
        indata_down_a = vmul(indata_down_a, Mask_111000, MASKREAD_OFF, 256);
        indata_down_b = vmul(indata_down_b, Mask_000111, MASKREAD_OFF, 256);

        temp_a = vsadd(indata_up_a, indata_up_b, MASKREAD_OFF, 256);
        temp_b = vsadd(indata_down_a, indata_down_b, MASKREAD_OFF, 256);
        vshuffle(temp_c, copy_index, R_Matrix_1_up, SHUFFLE_GATHER, 256);
        vshuffle(temp_d, copy_index, R_Matrix_1_down, SHUFFLE_GATHER, 256);
      } else if (column == 1) {
        vshuffle(indata_up_a, index_front_even, L_Matrix_1_up, SHUFFLE_GATHER,
                 256); // even = {0,2,4,6,...}
        vshuffle(indata_up_b, index_back_even, L_Matrix_1_down, SHUFFLE_GATHER, 256);
        vshuffle(indata_down_a, index_front_odd, L_Matrix_1_up, SHUFFLE_GATHER, 256);
        vshuffle(indata_down_b, index_back_odd, L_Matrix_1_down, SHUFFLE_GATHER, 256);
        indata_up_a   = vmul(indata_up_a, Mask_111000, MASKREAD_OFF, 256);
        indata_up_b   = vmul(indata_up_b, Mask_000111, MASKREAD_OFF, 256);
        indata_down_a = vmul(indata_down_a, Mask_111000, MASKREAD_OFF, 256);
        indata_down_b = vmul(indata_down_b, Mask_000111, MASKREAD_OFF, 256);

        temp_a = vsadd(indata_up_a, indata_up_b, MASKREAD_OFF, 256);
        temp_b = vsadd(indata_down_a, indata_down_b, MASKREAD_OFF, 256);
        vshuffle(temp_c, copy_index, R_Matrix_0_up, SHUFFLE_GATHER, 256);
        vshuffle(temp_d, copy_index, R_Matrix_0_down, SHUFFLE_GATHER, 256);
      }

      // 冗余
      temp_a = vsadd(temp_a, 0, MASKREAD_OFF, 256);
      temp_b = vsadd(temp_b, 0, MASKREAD_OFF, 256);
      temp_c = vsadd(temp_c, 0, MASKREAD_OFF, 256);
      temp_d = vsadd(temp_d, 0, MASKREAD_OFF, 256);

      __v4096i8 b_add_d;
      b_add_d = vsadd(temp_b, temp_d, MASKREAD_OFF, 256);

      __v4096i8 temp_sign_larger_0;
      __v4096i8 temp_sign_smaller_0;
      temp_sign_smaller_0 = vsgt(b_add_d, 0, MASKREAD_OFF, MASKWRITE_OFF, 256);
      temp_sign_larger_0  = vslt(b_add_d, 0, MASKREAD_OFF, MASKWRITE_OFF, 256);

      __v4096i8 sign_b_add_d;
      sign_b_add_d = vsadd(temp_sign_larger_0, 0, MASKREAD_OFF, 256);
      sign_b_add_d = vssub(temp_sign_smaller_0, sign_b_add_d, MASKREAD_OFF, 256);

      vbrdcst(temp_sign_larger_0, 0, MASKREAD_OFF, 256);
      vbrdcst(temp_sign_smaller_0, 0, MASKREAD_OFF, 256);

      temp_sign_smaller_0 = vsgt(temp_a, 0, MASKREAD_OFF, MASKWRITE_OFF, 256);
      temp_sign_larger_0  = vslt(temp_a, 0, MASKREAD_OFF, MASKWRITE_OFF, 256);

      __v4096i8 sign_a;
      sign_a = vsadd(temp_sign_larger_0, 0, MASKREAD_OFF, 256);
      sign_a = vssub(temp_sign_smaller_0, sign_a, MASKREAD_OFF, 256);

      __v4096i8 result_up_temp;
      __v4096i8 result_up_temp_a;
      result_up_temp_a = vmul(sign_a, sign_b_add_d, MASKREAD_OFF, 256);
      result_up_temp   = vmul(result_up_temp_a, alpha, MASKREAD_OFF, 256);

      __v4096i8 abs_b_add_d;
      __v4096i8 abs_a;
      vsgt(b_add_d, 0, MASKREAD_OFF, MASKWRITE_ON, 256);
      vbrdcst(temp, 0xFF, MASKREAD_OFF, 256);
      vbrdcst(temp1, 0x1, MASKREAD_OFF, 256);
      abs_b_add_d = vxor(temp, b_add_d, MASKREAD_ON, 256);
      abs_b_add_d = vsadd(temp1, abs_b_add_d, MASKREAD_ON, 256);
      // abs_b_add_d = vmul(b_add_d, -1, MASKREAD_ON, MASKWRITE_OFF, 256);
      vsgt(temp_a, 0, MASKREAD_OFF, MASKWRITE_ON, 256);
      abs_a = vxor(temp, temp_a, MASKREAD_ON, 256);
      abs_a = vsadd(temp1, abs_a, MASKREAD_ON, 256);
      // abs_a = vmul(temp_a, -1, MASKREAD_ON, MASKWRITE_OFF, 256);

      __v4096i8 min_bd_a;
      min_bd_a = vmul(abs_a, 0, MASKREAD_OFF, 256); // 冗余
      vsle(abs_b_add_d, abs_a, MASKREAD_OFF, MASKWRITE_ON, 256);
      min_bd_a = vsadd(abs_a, min_bd_a, MASKREAD_ON, 256);
      vsgt(abs_b_add_d, abs_a, MASKREAD_OFF, MASKWRITE_ON, 256);
      min_bd_a = vsadd(abs_b_add_d, min_bd_a, MASKREAD_ON, MASKWRITE_OFF, 256);

      result_up_temp = MUL4096_8_FIXED(result_up_temp, min_bd_a, 6, 256);

      //  down
      temp_sign_smaller_0 = vsgt(temp_c, 0, MASKREAD_OFF, MASKWRITE_OFF, 256);
      temp_sign_larger_0  = vslt(temp_c, 0, MASKREAD_OFF, MASKWRITE_OFF, 256);

      __v4096i8 sign_c;
      sign_c = vsadd(temp_sign_larger_0, 0, MASKREAD_OFF, 256);
      sign_c = vssub(temp_sign_smaller_0, sign_c, MASKREAD_OFF, 256);

      __v4096i8 result_down_temp;
      __v4096i8 result_down_temp_a;
      result_down_temp_a = vmul(sign_a, sign_c, MASKREAD_OFF, 256);
      result_down_temp   = vmul(result_down_temp_a, alpha, MASKREAD_OFF, 256);

      __v4096i8 abs_c;
      vsgt(temp_c, 0, MASKREAD_OFF, MASKWRITE_ON, 256);
      abs_c = vxor(temp, temp_c, MASKREAD_ON, 256);
      abs_c = vsadd(temp1, abs_c, MASKREAD_ON, 256);
      // abs_c = vmul(temp_c, -1, MASKREAD_ON, MASKWRITE_OFF, 256);

      __v4096i8 min_a_c;
      min_a_c = vmul(abs_c, 0, MASKREAD_OFF, 256); //  冗余
      vsle(abs_a, abs_c, MASKREAD_OFF, MASKWRITE_ON, 256);
      min_a_c = vsadd(abs_c, min_a_c, MASKREAD_ON, MASKWRITE_OFF, 256);
      vsgt(abs_a, abs_c, MASKREAD_OFF, MASKWRITE_ON, 256);
      min_a_c = vsadd(abs_a, min_a_c, MASKREAD_ON, MASKWRITE_OFF, 256);

      result_down_temp = MUL4096_8_FIXED(result_down_temp, min_a_c, 6, 256);
      result_down_temp = vsadd(result_down_temp, temp_b, MASKREAD_OFF, 256);

      if (column == 9) {
        L_Matrix_8_up   = vmul(L_Matrix_8_up, 0, MASKREAD_OFF, 256);   //  冗余
        L_Matrix_8_down = vmul(L_Matrix_8_down, 0, MASKREAD_OFF, 256); //  冗余
        vshuffle(L_Matrix_8_up, copy_index, result_up_temp, SHUFFLE_GATHER, 256);
        vshuffle(L_Matrix_8_down, copy_index, result_down_temp, SHUFFLE_GATHER, 256);
      } else if (column == 8) {
        L_Matrix_7_up   = vmul(L_Matrix_7_up, 0, MASKREAD_OFF, 256);   //  冗余
        L_Matrix_7_down = vmul(L_Matrix_7_down, 0, MASKREAD_OFF, 256); //  冗余
        vshuffle(L_Matrix_7_up, copy_index, result_up_temp, SHUFFLE_GATHER, 256);
        vshuffle(L_Matrix_7_down, copy_index, result_down_temp, SHUFFLE_GATHER, 256);
      } else if (column == 7) {
        L_Matrix_6_up   = vmul(L_Matrix_6_up, 0, MASKREAD_OFF, 256);   //  冗余
        L_Matrix_6_down = vmul(L_Matrix_6_down, 0, MASKREAD_OFF, 256); //  冗余
        vshuffle(L_Matrix_6_up, copy_index, result_up_temp, SHUFFLE_GATHER, 256);
        vshuffle(L_Matrix_6_down, copy_index, result_down_temp, SHUFFLE_GATHER, 256);
      } else if (column == 6) {
        L_Matrix_5_up   = vmul(L_Matrix_5_up, 0, MASKREAD_OFF, 256);   //  冗余
        L_Matrix_5_down = vmul(L_Matrix_5_down, 0, MASKREAD_OFF, 256); //  冗余
        vshuffle(L_Matrix_5_up, copy_index, result_up_temp, SHUFFLE_GATHER, 256);
        vshuffle(L_Matrix_5_down, copy_index, result_down_temp, SHUFFLE_GATHER, 256);
      } else if (column == 5) {
        L_Matrix_4_up   = vmul(L_Matrix_4_up, 0, MASKREAD_OFF, 256);   //  冗余
        L_Matrix_4_down = vmul(L_Matrix_4_down, 0, MASKREAD_OFF, 256); //  冗余
        vshuffle(L_Matrix_4_up, copy_index, result_up_temp, SHUFFLE_GATHER, 256);
        vshuffle(L_Matrix_4_down, copy_index, result_down_temp, SHUFFLE_GATHER, 256);
      } else if (column == 4) {
        L_Matrix_3_up   = vmul(L_Matrix_3_up, 0, MASKREAD_OFF, 256);   //  冗余
        L_Matrix_3_down = vmul(L_Matrix_3_down, 0, MASKREAD_OFF, 256); //  冗余
        vshuffle(L_Matrix_3_up, copy_index, result_up_temp, SHUFFLE_GATHER, 256);
        vshuffle(L_Matrix_3_down, copy_index, result_down_temp, SHUFFLE_GATHER, 256);
      } else if (column == 3) {
        L_Matrix_2_up   = vmul(L_Matrix_2_up, 0, MASKREAD_OFF, 256);   //  冗余
        L_Matrix_2_down = vmul(L_Matrix_2_down, 0, MASKREAD_OFF, 256); //  冗余
        vshuffle(L_Matrix_2_up, copy_index, result_up_temp, SHUFFLE_GATHER, 256);
        vshuffle(L_Matrix_2_down, copy_index, result_down_temp, SHUFFLE_GATHER, 256);
      } else if (column == 2) {
        L_Matrix_1_up   = vmul(L_Matrix_1_up, 0, MASKREAD_OFF, 256);   //  冗余
        L_Matrix_1_down = vmul(L_Matrix_1_down, 0, MASKREAD_OFF, 256); //  冗余
        vshuffle(L_Matrix_1_up, copy_index, result_up_temp, SHUFFLE_GATHER, 256);
        vshuffle(L_Matrix_1_down, copy_index, result_down_temp, SHUFFLE_GATHER, 256);
      } else if (column == 1) {
        L_Matrix_0_up   = vmul(L_Matrix_0_up, 0, MASKREAD_OFF, 256);   //  冗余
        L_Matrix_0_down = vmul(L_Matrix_0_down, 0, MASKREAD_OFF, 256); //  冗余
        vshuffle(L_Matrix_0_up, copy_index, result_up_temp, SHUFFLE_GATHER, 256);
        vshuffle(L_Matrix_0_down, copy_index, result_down_temp, SHUFFLE_GATHER, 256);
      }
    }

    //  R Matrix Calculation
    for (int column = 1; column < 10; column++) {
      __v4096i8 indata_up_a;
      __v4096i8 indata_up_b;
      __v4096i8 indata_down_a;
      __v4096i8 indata_down_b;
      indata_up_a   = vmul(alpha, 0, MASKREAD_OFF, 256);
      indata_up_b   = vmul(alpha, 0, MASKREAD_OFF, 256);
      indata_down_a = vmul(alpha, 0, MASKREAD_OFF, 256);
      indata_down_b = vmul(alpha, 0, MASKREAD_OFF, 256);
      if (column == 1) {
        vshuffle(indata_up_a, index_front_even, L_Matrix_1_up, SHUFFLE_GATHER,
                 256); // even = {0,2,4,6,...}
        vshuffle(indata_up_b, index_back_even, L_Matrix_1_down, SHUFFLE_GATHER, 256);
        vshuffle(indata_down_a, index_front_odd, L_Matrix_1_up, SHUFFLE_GATHER, 256);
        vshuffle(indata_down_b, index_back_odd, L_Matrix_1_down, SHUFFLE_GATHER, 256);
        indata_up_a   = vmul(indata_up_a, Mask_111000, MASKREAD_OFF, 256);
        indata_up_b   = vmul(indata_up_b, Mask_000111, MASKREAD_OFF, 256);
        indata_down_a = vmul(indata_down_a, Mask_111000, MASKREAD_OFF, 256);
        indata_down_b = vmul(indata_down_b, Mask_000111, MASKREAD_OFF, 256);

        temp_a = vsadd(indata_up_a, indata_up_b, MASKREAD_OFF, 256);
        temp_b = vsadd(indata_down_a, indata_down_b, MASKREAD_OFF, 256);
        vshuffle(temp_c, copy_index, R_Matrix_0_up, SHUFFLE_GATHER, 256);
        vshuffle(temp_d, copy_index, R_Matrix_0_down, SHUFFLE_GATHER, 256);
      } else if (column == 2) {
        vshuffle(indata_up_a, index_front_even, L_Matrix_2_up, SHUFFLE_GATHER,
                 256); // even = {0,2,4,6,...}
        vshuffle(indata_up_b, index_back_even, L_Matrix_2_down, SHUFFLE_GATHER, 256);
        vshuffle(indata_down_a, index_front_odd, L_Matrix_2_up, SHUFFLE_GATHER, 256);
        vshuffle(indata_down_b, index_back_odd, L_Matrix_2_down, SHUFFLE_GATHER, 256);
        indata_up_a   = vmul(indata_up_a, Mask_111000, MASKREAD_OFF, 256);
        indata_up_b   = vmul(indata_up_b, Mask_000111, MASKREAD_OFF, 256);
        indata_down_a = vmul(indata_down_a, Mask_111000, MASKREAD_OFF, 256);
        indata_down_b = vmul(indata_down_b, Mask_000111, MASKREAD_OFF, 256);

        temp_a = vsadd(indata_up_a, indata_up_b, MASKREAD_OFF, 256);
        temp_b = vsadd(indata_down_a, indata_down_b, MASKREAD_OFF, 256);
        vshuffle(temp_c, copy_index, R_Matrix_1_up, SHUFFLE_GATHER, 256);
        vshuffle(temp_d, copy_index, R_Matrix_1_down, SHUFFLE_GATHER, 256);
      } else if (column == 3) {
        vshuffle(indata_up_a, index_front_even, L_Matrix_3_up, SHUFFLE_GATHER,
                 256); // even = {0,2,4,6,...}
        vshuffle(indata_up_b, index_back_even, L_Matrix_3_down, SHUFFLE_GATHER, 256);
        vshuffle(indata_down_a, index_front_odd, L_Matrix_3_up, SHUFFLE_GATHER, 256);
        vshuffle(indata_down_b, index_back_odd, L_Matrix_3_down, SHUFFLE_GATHER, 256);
        indata_up_a   = vmul(indata_up_a, Mask_111000, MASKREAD_OFF, 256);
        indata_up_b   = vmul(indata_up_b, Mask_000111, MASKREAD_OFF, 256);
        indata_down_a = vmul(indata_down_a, Mask_111000, MASKREAD_OFF, 256);
        indata_down_b = vmul(indata_down_b, Mask_000111, MASKREAD_OFF, 256);

        temp_a = vsadd(indata_up_a, indata_up_b, MASKREAD_OFF, 256);
        temp_b = vsadd(indata_down_a, indata_down_b, MASKREAD_OFF, 256);
        vshuffle(temp_c, copy_index, R_Matrix_2_up, SHUFFLE_GATHER, 256);
        vshuffle(temp_d, copy_index, R_Matrix_2_down, SHUFFLE_GATHER, 256);
      } else if (column == 4) {
        vshuffle(indata_up_a, index_front_even, L_Matrix_4_up, SHUFFLE_GATHER,
                 256); // even = {0,2,4,6,...}
        vshuffle(indata_up_b, index_back_even, L_Matrix_4_down, SHUFFLE_GATHER, 256);
        vshuffle(indata_down_a, index_front_odd, L_Matrix_4_up, SHUFFLE_GATHER, 256);
        vshuffle(indata_down_b, index_back_odd, L_Matrix_4_down, SHUFFLE_GATHER, 256);
        indata_up_a   = vmul(indata_up_a, Mask_111000, MASKREAD_OFF, 256);
        indata_up_b   = vmul(indata_up_b, Mask_000111, MASKREAD_OFF, 256);
        indata_down_a = vmul(indata_down_a, Mask_111000, MASKREAD_OFF, 256);
        indata_down_b = vmul(indata_down_b, Mask_000111, MASKREAD_OFF, 256);

        temp_a = vsadd(indata_up_a, indata_up_b, MASKREAD_OFF, 256);
        temp_b = vsadd(indata_down_a, indata_down_b, MASKREAD_OFF, 256);
        vshuffle(temp_c, copy_index, R_Matrix_3_up, SHUFFLE_GATHER, 256);
        vshuffle(temp_d, copy_index, R_Matrix_3_down, SHUFFLE_GATHER, 256);
      } else if (column == 5) {
        vshuffle(indata_up_a, index_front_even, L_Matrix_5_up, SHUFFLE_GATHER,
                 256); // even = {0,2,4,6,...}
        vshuffle(indata_up_b, index_back_even, L_Matrix_5_down, SHUFFLE_GATHER, 256);
        vshuffle(indata_down_a, index_front_odd, L_Matrix_5_up, SHUFFLE_GATHER, 256);
        vshuffle(indata_down_b, index_back_odd, L_Matrix_5_down, SHUFFLE_GATHER, 256);
        indata_up_a   = vmul(indata_up_a, Mask_111000, MASKREAD_OFF, 256);
        indata_up_b   = vmul(indata_up_b, Mask_000111, MASKREAD_OFF, 256);
        indata_down_a = vmul(indata_down_a, Mask_111000, MASKREAD_OFF, 256);
        indata_down_b = vmul(indata_down_b, Mask_000111, MASKREAD_OFF, 256);

        temp_a = vsadd(indata_up_a, indata_up_b, MASKREAD_OFF, 256);
        temp_b = vsadd(indata_down_a, indata_down_b, MASKREAD_OFF, 256);
        vshuffle(temp_c, copy_index, R_Matrix_4_up, SHUFFLE_GATHER, 256);
        vshuffle(temp_d, copy_index, R_Matrix_4_down, SHUFFLE_GATHER, 256);
      } else if (column == 6) {
        vshuffle(indata_up_a, index_front_even, L_Matrix_6_up, SHUFFLE_GATHER,
                 256); // even = {0,2,4,6,...}
        vshuffle(indata_up_b, index_back_even, L_Matrix_6_down, SHUFFLE_GATHER, 256);
        vshuffle(indata_down_a, index_front_odd, L_Matrix_6_up, SHUFFLE_GATHER, 256);
        vshuffle(indata_down_b, index_back_odd, L_Matrix_6_down, SHUFFLE_GATHER, 256);
        indata_up_a   = vmul(indata_up_a, Mask_111000, MASKREAD_OFF, 256);
        indata_up_b   = vmul(indata_up_b, Mask_000111, MASKREAD_OFF, 256);
        indata_down_a = vmul(indata_down_a, Mask_111000, MASKREAD_OFF, 256);
        indata_down_b = vmul(indata_down_b, Mask_000111, MASKREAD_OFF, 256);

        temp_a = vsadd(indata_up_a, indata_up_b, MASKREAD_OFF, 256);
        temp_b = vsadd(indata_down_a, indata_down_b, MASKREAD_OFF, 256);
        vshuffle(temp_c, copy_index, R_Matrix_5_up, SHUFFLE_GATHER, 256);
        vshuffle(temp_d, copy_index, R_Matrix_5_down, SHUFFLE_GATHER, 256);
      } else if (column == 7) {
        vshuffle(indata_up_a, index_front_even, L_Matrix_7_up, SHUFFLE_GATHER,
                 256); // even = {0,2,4,6,...}
        vshuffle(indata_up_b, index_back_even, L_Matrix_7_down, SHUFFLE_GATHER, 256);
        vshuffle(indata_down_a, index_front_odd, L_Matrix_7_up, SHUFFLE_GATHER, 256);
        vshuffle(indata_down_b, index_back_odd, L_Matrix_7_down, SHUFFLE_GATHER, 256);
        indata_up_a   = vmul(indata_up_a, Mask_111000, MASKREAD_OFF, 256);
        indata_up_b   = vmul(indata_up_b, Mask_000111, MASKREAD_OFF, 256);
        indata_down_a = vmul(indata_down_a, Mask_111000, MASKREAD_OFF, 256);
        indata_down_b = vmul(indata_down_b, Mask_000111, MASKREAD_OFF, 256);

        temp_a = vsadd(indata_up_a, indata_up_b, MASKREAD_OFF, 256);
        temp_b = vsadd(indata_down_a, indata_down_b, MASKREAD_OFF, 256);
        vshuffle(temp_c, copy_index, R_Matrix_6_up, SHUFFLE_GATHER, 256);
        vshuffle(temp_d, copy_index, R_Matrix_6_down, SHUFFLE_GATHER, 256);
      } else if (column == 8) {
        vshuffle(indata_up_a, index_front_even, L_Matrix_8_up, SHUFFLE_GATHER,
                 256); // even = {0,2,4,6,...}
        vshuffle(indata_up_b, index_back_even, L_Matrix_8_down, SHUFFLE_GATHER, 256);
        vshuffle(indata_down_a, index_front_odd, L_Matrix_8_up, SHUFFLE_GATHER, 256);
        vshuffle(indata_down_b, index_back_odd, L_Matrix_8_down, SHUFFLE_GATHER, 256);
        indata_up_a   = vmul(indata_up_a, Mask_111000, MASKREAD_OFF, 256);
        indata_up_b   = vmul(indata_up_b, Mask_000111, MASKREAD_OFF, 256);
        indata_down_a = vmul(indata_down_a, Mask_111000, MASKREAD_OFF, 256);
        indata_down_b = vmul(indata_down_b, Mask_000111, MASKREAD_OFF, 256);

        temp_a = vsadd(indata_up_a, indata_up_b, MASKREAD_OFF, 256);
        temp_b = vsadd(indata_down_a, indata_down_b, MASKREAD_OFF, 256);
        vshuffle(temp_c, copy_index, R_Matrix_7_up, SHUFFLE_GATHER, 256);
        vshuffle(temp_d, copy_index, R_Matrix_7_down, SHUFFLE_GATHER, 256);
      } else if (column == 9) {
        vshuffle(indata_up_a, index_front_even, L_Matrix_N_up, SHUFFLE_GATHER,
                 256); // even = {0,2,4,6,...}
        vshuffle(indata_up_b, index_back_even, L_Matrix_N_down, SHUFFLE_GATHER, 256);
        vshuffle(indata_down_a, index_front_odd, L_Matrix_N_up, SHUFFLE_GATHER, 256);
        vshuffle(indata_down_b, index_back_odd, L_Matrix_N_down, SHUFFLE_GATHER, 256);
        indata_up_a   = vmul(indata_up_a, Mask_111000, MASKREAD_OFF, 256);
        indata_up_b   = vmul(indata_up_b, Mask_000111, MASKREAD_OFF, 256);
        indata_down_a = vmul(indata_down_a, Mask_111000, MASKREAD_OFF, 256);
        indata_down_b = vmul(indata_down_b, Mask_000111, MASKREAD_OFF, 256);

        temp_a = vsadd(indata_up_a, indata_up_b, MASKREAD_OFF, 256);
        temp_b = vsadd(indata_down_a, indata_down_b, MASKREAD_OFF, 256);
        vshuffle(temp_c, copy_index, R_Matrix_8_up, SHUFFLE_GATHER, 256);
        vshuffle(temp_d, copy_index, R_Matrix_8_down, SHUFFLE_GATHER, 256);
      }

      __v4096i8 b_add_d;
      b_add_d = vsadd(temp_b, temp_d, MASKREAD_OFF, 256);

      __v4096i8 temp_sign_larger_0;
      __v4096i8 temp_sign_smaller_0;
      temp_sign_smaller_0 = vsgt(b_add_d, 0, MASKREAD_OFF, MASKWRITE_OFF, 256);
      temp_sign_larger_0  = vslt(b_add_d, 0, MASKREAD_OFF, MASKWRITE_OFF, 256);

      __v4096i8 sign_b_add_d;
      sign_b_add_d = vsadd(temp_sign_larger_0, 0, MASKREAD_OFF, 256);
      sign_b_add_d = vssub(temp_sign_smaller_0, sign_b_add_d, MASKREAD_OFF, 256);

      vbrdcst(temp_sign_larger_0, 0, MASKREAD_OFF, 256);
      vbrdcst(temp_sign_smaller_0, 0, MASKREAD_OFF, 256);

      temp_sign_smaller_0 = vsgt(temp_c, 0, MASKREAD_OFF, MASKWRITE_OFF, 256);
      temp_sign_larger_0  = vslt(temp_c, 0, MASKREAD_OFF, MASKWRITE_OFF, 256);

      __v4096i8 sign_c;
      sign_c = vsadd(temp_sign_larger_0, 0, MASKREAD_OFF, 256);
      sign_c = vssub(temp_sign_smaller_0, sign_c, MASKREAD_OFF, 256);

      // __v4096i8 result_up_temp_a;
      result_up_temp_a = vmul(sign_c, sign_b_add_d, MASKREAD_OFF, 256);
      result_up_temp   = vmul(result_up_temp_a, alpha, MASKREAD_OFF, 256);

      __v4096i8 abs_b_add_d;
      __v4096i8 abs_c;
      vsgt(b_add_d, 0, MASKREAD_OFF, MASKWRITE_ON, 256);
      abs_b_add_d = vxor(temp, b_add_d, MASKREAD_ON, 256);
      abs_b_add_d = vsadd(temp1, abs_b_add_d, MASKREAD_ON, 256);
      // abs_b_add_d = vmul(b_add_d, -1, MASKREAD_ON, MASKWRITE_OFF, 256);
      vsgt(temp_c, 0, MASKREAD_OFF, MASKWRITE_ON, 256);
      abs_c = vxor(temp, temp_c, MASKREAD_ON, 256);
      abs_c = vsadd(temp1, abs_c, MASKREAD_ON, 256);
      // abs_a = vmul(temp_a, -1, MASKREAD_ON, MASKWRITE_OFF, 256);

      // vsgt(b_add_d, 0, MASKREAD_OFF, MASKWRITE_ON, 256);
      // abs_b_add_d = vmul(b_add_d, -1, MASKREAD_ON, MASKWRITE_OFF, 256);
      // vsgt(temp_c, 0, MASKREAD_OFF, MASKWRITE_ON, 256);
      // abs_c = vmul(temp_c, -1, MASKREAD_ON, MASKWRITE_OFF, 256);

      // __v4096i8 min_bd_c;
      min_bd_c = vmul(abs_c, 0, MASKREAD_OFF, 256); // 冗余
      vsle(abs_b_add_d, abs_c, MASKREAD_OFF, MASKWRITE_ON, 256);
      min_bd_c = vsadd(abs_c, min_bd_c, MASKREAD_ON, 256);
      vsgt(abs_b_add_d, abs_c, MASKREAD_OFF, MASKWRITE_ON, 256);
      min_bd_c = vsadd(abs_b_add_d, min_bd_c, MASKREAD_ON, MASKWRITE_OFF, 256);

      result_up_temp = MUL4096_8_FIXED(result_up_temp, min_bd_c, 6, 256);

      //  down
      temp_sign_smaller_0 = vsgt(temp_a, 0, MASKREAD_OFF, MASKWRITE_OFF, 256);
      temp_sign_larger_0  = vslt(temp_a, 0, MASKREAD_OFF, MASKWRITE_OFF, 256);

      __v4096i8 sign_a;
      sign_a = vsadd(temp_sign_larger_0, 0, MASKREAD_OFF, 256);
      sign_a = vssub(temp_sign_smaller_0, sign_a, MASKREAD_OFF, 256);

      __v4096i8 result_down_temp;
      __v4096i8 result_down_temp_a;
      result_down_temp_a = vmul(sign_a, sign_c, MASKREAD_OFF, 256);
      result_down_temp   = vmul(result_down_temp_a, alpha, MASKREAD_OFF, 256);

      __v4096i8 abs_a;
      vsgt(temp_a, 0, MASKREAD_OFF, MASKWRITE_ON, 256);
      abs_a = vxor(temp, temp_a, MASKREAD_ON, 256);
      abs_a = vsadd(temp1, abs_a, MASKREAD_ON, 256);
      // abs_a = vmul(temp_a, -1, MASKREAD_ON, MASKWRITE_OFF, 256);

      __v4096i8 min_a_c;
      min_a_c = vmul(abs_c, 0, MASKREAD_OFF, 256); //  冗余
      vsle(abs_a, abs_c, MASKREAD_OFF, MASKWRITE_ON, 256);
      min_a_c = vsadd(abs_c, min_a_c, MASKREAD_ON, MASKWRITE_OFF, 256);
      vsgt(abs_a, abs_c, MASKREAD_OFF, MASKWRITE_ON, 256);
      min_a_c = vsadd(abs_a, min_a_c, MASKREAD_ON, MASKWRITE_OFF, 256);

      result_down_temp = MUL4096_8_FIXED(result_down_temp, min_a_c, 6, 256);
      result_down_temp = vsadd(result_down_temp, temp_d, MASKREAD_OFF, 256);

      __v4096i8 result_up;
      __v4096i8 result_down;
      __v4096i8 result_up_a;
      __v4096i8 result_up_b;
      __v4096i8 result_down_a;
      __v4096i8 result_down_b;

      //  冗余
      result_up_a   = vmul(result_down_temp, 0, MASKREAD_OFF, 256);
      result_up_b   = vmul(result_down_temp, 0, MASKREAD_OFF, 256);
      result_down_a = vmul(result_down_temp, 0, MASKREAD_OFF, 256);
      result_down_b = vmul(result_down_temp, 0, MASKREAD_OFF, 256);

      vshuffle(result_up_a, Xor_index_up, result_up_temp, SHUFFLE_GATHER, 256);
      vshuffle(result_up_b, Equal_index_up, result_down_temp, SHUFFLE_GATHER, 256);
      vshuffle(result_down_a, Xor_index_down, result_up_temp, SHUFFLE_GATHER, 256);
      vshuffle(result_down_b, Equal_index_down, result_down_temp, SHUFFLE_GATHER, 256);
      result_up_a   = vmul(result_up_a, Mask_1010, MASKREAD_OFF, 256);
      result_up_b   = vmul(result_up_b, Mask_0101, MASKREAD_OFF, 256);
      result_down_a = vmul(result_down_a, Mask_1010, MASKREAD_OFF, 256);
      result_down_b = vmul(result_down_b, Mask_0101, MASKREAD_OFF, 256);
      result_up     = vsadd(result_up_a, result_up_b, MASKREAD_OFF, 256);
      result_down   = vsadd(result_down_a, result_down_b, MASKREAD_OFF, 256);

      if (column == 9) {
        R_Matrix_9_up   = vmul(alpha, 0, MASKREAD_OFF, 256); // 冗余
        R_Matrix_9_down = vmul(alpha, 0, MASKREAD_OFF, 256); // 冗余
        vshuffle(R_Matrix_9_up, copy_index, result_up, SHUFFLE_GATHER, 256);
        vshuffle(R_Matrix_9_down, copy_index, result_down, SHUFFLE_GATHER, 256);
      } else if (column == 8) {
        R_Matrix_8_up   = vmul(alpha, 0, MASKREAD_OFF, 256); // 冗余
        R_Matrix_8_down = vmul(alpha, 0, MASKREAD_OFF, 256); // 冗余
        vshuffle(R_Matrix_8_up, copy_index, result_up, SHUFFLE_GATHER, 256);
        vshuffle(R_Matrix_8_down, copy_index, result_down, SHUFFLE_GATHER, 256);
      } else if (column == 7) {
        R_Matrix_7_up   = vmul(alpha, 0, MASKREAD_OFF, 256); // 冗余
        R_Matrix_7_down = vmul(alpha, 0, MASKREAD_OFF, 256); // 冗余
        vshuffle(R_Matrix_7_up, copy_index, result_up, SHUFFLE_GATHER, 256);
        vshuffle(R_Matrix_7_down, copy_index, result_down, SHUFFLE_GATHER, 256);
      } else if (column == 6) {
        R_Matrix_6_up   = vmul(alpha, 0, MASKREAD_OFF, 256); // 冗余
        R_Matrix_6_down = vmul(alpha, 0, MASKREAD_OFF, 256); // 冗余
        vshuffle(R_Matrix_6_up, copy_index, result_up, SHUFFLE_GATHER, 256);
        vshuffle(R_Matrix_6_down, copy_index, result_down, SHUFFLE_GATHER, 256);
      } else if (column == 5) {
        R_Matrix_5_up   = vmul(alpha, 0, MASKREAD_OFF, 256); // 冗余
        R_Matrix_5_down = vmul(alpha, 0, MASKREAD_OFF, 256); // 冗余
        vshuffle(R_Matrix_5_up, copy_index, result_up, SHUFFLE_GATHER, 256);
        vshuffle(R_Matrix_5_down, copy_index, result_down, SHUFFLE_GATHER, 256);
      } else if (column == 4) {
        R_Matrix_4_up   = vmul(alpha, 0, MASKREAD_OFF, 256); // 冗余
        R_Matrix_4_down = vmul(alpha, 0, MASKREAD_OFF, 256); // 冗余
        vshuffle(R_Matrix_4_up, copy_index, result_up, SHUFFLE_GATHER, 256);
        vshuffle(R_Matrix_4_down, copy_index, result_down, SHUFFLE_GATHER, 256);
      } else if (column == 3) {
        R_Matrix_3_up   = vmul(alpha, 0, MASKREAD_OFF, 256); // 冗余
        R_Matrix_3_down = vmul(alpha, 0, MASKREAD_OFF, 256); // 冗余
        vshuffle(R_Matrix_3_up, copy_index, result_up, SHUFFLE_GATHER, 256);
        vshuffle(R_Matrix_3_down, copy_index, result_down, SHUFFLE_GATHER, 256);
      } else if (column == 2) {
        R_Matrix_2_up   = vmul(alpha, 0, MASKREAD_OFF, 256); // 冗余
        R_Matrix_2_down = vmul(alpha, 0, MASKREAD_OFF, 256); // 冗余
        vshuffle(R_Matrix_2_up, copy_index, result_up, SHUFFLE_GATHER, 256);
        vshuffle(R_Matrix_2_down, copy_index, result_down, SHUFFLE_GATHER, 256);
      } else if (column == 1) {
        R_Matrix_1_up   = vmul(alpha, 0, MASKREAD_OFF, 256); // 冗余
        R_Matrix_1_down = vmul(alpha, 0, MASKREAD_OFF, 256); // 冗余
        vshuffle(R_Matrix_1_up, copy_index, result_up, SHUFFLE_GATHER, 256);
        vshuffle(R_Matrix_1_down, copy_index, result_down, SHUFFLE_GATHER, 256);
      }
    }
  }
  __v4096i8 final_llr_up;
  __v4096i8 final_llr_down;
  final_llr_up   = vsadd(L_Matrix_0_up, R_Matrix_0_up, MASKREAD_OFF, 256);
  final_llr_down = vsadd(L_Matrix_0_down, R_Matrix_0_down, MASKREAD_OFF, 256);

  __v4096i8 final_llr;
  vbrdcst(final_llr, 0, MASKREAD_OFF, 512);
  __v2048i16 move_256to512;
  __v2048i16 mask_0to255;
  vrange(move_256to512, 512);
  move_256to512 = vrsub(move_256to512, 256, MASKREAD_OFF, 512);
  mask_0to255   = vmul(move_256to512, 0, MASKREAD_OFF, 512); // 初始化乘0
  mask_0to255   = vsadd(mask_0to255, 1, MASKREAD_OFF, 512);  // 全部赋值为1
  mask_0to255   = vrsub(mask_0to255, 1, MASKREAD_OFF, 256);  // 前一半为0
  move_256to512 = vmul(move_256to512, mask_0to255, MASKREAD_OFF, 512);

  vshuffle(final_llr, move_256to512, final_llr_down, SHUFFLE_GATHER, 512);
  __v4096i8 mask_0to255_8bit;
  vbrdcst(mask_0to255_8bit, 1, MASKREAD_OFF, 512);
  vbrdcst(mask_0to255_8bit, 0, MASKREAD_OFF, 256);
  final_llr = vmul(final_llr, mask_0to255_8bit, MASKREAD_OFF, 512);
  vbrdcst(mask_0to255_8bit, 0, MASKREAD_OFF, 512);
  vbrdcst(mask_0to255_8bit, 1, MASKREAD_OFF, 256);
  final_llr_up = vmul(final_llr_up, mask_0to255_8bit, MASKREAD_OFF, 512);

  final_llr = vsadd(final_llr, final_llr_up, MASKREAD_OFF, 512);

  __v4096i8 final_bit;
  final_bit = vsgt(final_llr, 0, MASKREAD_OFF, MASKWRITE_OFF, 512);

  // __v2048i16 PBCH_bit_Index;
  // __v2048i16 iIL_index;
  // vclaim(PBCH_bit_Index);
  // vclaim(iIL_index);

  // vbarrier();
  // VSPM_OPEN();
  // int PBCH_bit_Index_addr = vaddr(PBCH_bit_Index);
  // for (int i = 0; i < 56; i++) {
  //   *(volatile unsigned short *)(PBCH_bit_Index_addr + (i << 1)) =
  //       PBCH_Info_index[i];
  // }
  // int iIL_index_addr = vaddr(iIL_index);
  // for (int i = 0; i < 56; i++) {
  //   *(volatile unsigned short *)(iIL_index_addr + (i << 1)) = iIL_56_PBCH[i];
  // }
  // VSPM_CLOSE();

  __v4096i8 PBCH_bits_out_temp;
  __v4096i8 PBCH_bits_out;
  vclaim(PBCH_bits_out_temp);
  vclaim(PBCH_bits_out);
  vshuffle(PBCH_bits_out_temp, PBCH_bit_Index, final_bit, SHUFFLE_GATHER, 56);
  vshuffle(PBCH_bits_out, iIL_index, PBCH_bits_out_temp, SHUFFLE_SCATTER, 56);

  vbrdcst(mask_0to255_8bit, 0, MASKREAD_OFF, 512);
  vbrdcst(mask_0to255_8bit, 1, MASKREAD_OFF, 56);
  PBCH_bits_out = vmul(PBCH_bits_out, mask_0to255_8bit, MASKREAD_OFF, 512);

  // __v4096i8 final_bits_up;
  // __v4096i8 final_bits_down;
  // final_bits_up = vsgt(final_llr_up, 0, MASKREAD_OFF, MASKWRITE_OFF, 256);
  // final_bits_down = vsgt(final_llr_down, 0, MASKREAD_OFF, MASKWRITE_OFF,
  // 256);

  // printf("\n\nPolar_Length:%hd\n\n", &Polar_Length);

  // VENUS_PRINTVEC_CHAR(min_bd_c, 256);
  return PBCH_bits_out;
}

VENUS_INLINE __v4096i8 crc_24C(__v4096i8 vin, int fullLen, __v4096i8 poly) {
  int pariLen = 24 + 1;
  int msgLen  = fullLen - pariLen + 1;
  int tmp;

  __v4096i8 buf;
  __v4096i8 msg;
  // __v4096i8 poly;
  __v2048i16 index;
  vclaim(buf);
  vclaim(msg);
  // vclaim(poly);
  vclaim(index);

  vrange(index, msgLen);
  vbrdcst(msg, 0, MASKREAD_OFF, fullLen);
  vshuffle(msg, index, vin, SHUFFLE_GATHER, msgLen);

  // int poly_addr = vaddr(poly);
  // vbarrier();
  // VSPM_OPEN();
  // for (size_t i = 0; i < pariLen; i++) {
  //   unsigned int addr = poly_addr + i;
  //   *(volatile unsigned char *)(addr) = POLY_CRC24C[i];
  // }
  // VSPM_CLOSE();

  for (int i = 0; i < msgLen; i++) {
    int m_addr = vaddr(msg);
    vbarrier();
    VSPM_OPEN();
    unsigned int addr = m_addr + i;
    tmp               = *(volatile unsigned char *)(addr);
    VSPM_CLOSE();

    if (tmp == 1) {
      vrange(index, pariLen);
      index = vsadd(index, i, MASKREAD_OFF, pariLen);
      vshuffle(buf, index, msg, SHUFFLE_GATHER, pariLen);
      buf = vxor(buf, poly, MASKREAD_OFF, pariLen);
      vshuffle(msg, index, buf, SHUFFLE_SCATTER, pariLen);
    }
  }

  vrange(index, pariLen - 1);
  index = vsadd(index, msgLen, MASKREAD_OFF, pariLen - 1);
  vshuffle(buf, index, msg, SHUFFLE_GATHER, pariLen - 1);

  return buf;
}

static uint8_t seqShuffleIndex[32];
static uint8_t aBarShuffleIndex[32];
static uint8_t G[32] = {16, 23, 18, 17, 8,  30, 10, 6,  24, 7,  0,  5,  3,  2,  1,  4,
                        9,  11, 12, 13, 14, 15, 19, 20, 21, 22, 25, 26, 27, 28, 29, 31};
static uint8_t isScrambled[32];

VENUS_INLINE __v4096i8 DescrambleAndDeInterleaverIndex(__v4096i8 scrBlk, __v4096i8 seqSet, short Lssb) {

  uint8_t jSFN = 7;
  uint8_t jHRF = 10;
  uint8_t jSSB = 11;
  uint8_t jOTH = 14;
  uint8_t M    = (Lssb == 64) ? 26 : 29;
  uint8_t v    = 0;

  int scrBlk_addr = vaddr(scrBlk);
  vbarrier();
  VSPM_OPEN();
  v += (*(volatile char *)(scrBlk_addr + G[jSFN + 1]));
  VSPM_CLOSE();
  scrBlk_addr = vaddr(scrBlk);
  vbarrier();
  VSPM_OPEN();
  v += (*(volatile char *)(scrBlk_addr + G[jSFN])) * 2;
  VSPM_CLOSE();

  __v2048i16 v_shuffle_index;
  vclaim(v_shuffle_index);
  vrange(v_shuffle_index, M);
  __v2048i16 const_value;
  vclaim(const_value);
  vbrdcst(const_value, v * M, MASKREAD_OFF, M);
  v_shuffle_index = vsadd(v_shuffle_index, const_value, MASKREAD_OFF, M);

  __v4096i8 seqSet_tmp;
  vclaim(seqSet_tmp);
  vshuffle(seqSet_tmp, v_shuffle_index, seqSet, SHUFFLE_GATHER, M);

  for (int i = 0; i < 32; ++i) {
    isScrambled[i] = 1;
  }

  jSFN = 0;
  for (int i = 0; i < 32; ++i) {
    if ((i > 0 && i <= 6) || (i >= 24 && i <= 27)) {
      if (i == 25 || i == 26)
        isScrambled[G[jSFN]] = 0;
      aBarShuffleIndex[i] = G[jSFN];
      jSFN++;
    } else if (i == 28) {
      aBarShuffleIndex[i]  = G[jHRF];
      isScrambled[G[jHRF]] = 0;
    } else if (i >= 29 && i <= 31) {
      if (Lssb == 64)
        isScrambled[G[jSSB]] = 0;
      aBarShuffleIndex[i] = G[jSSB];
      jSSB++;
    } else {
      aBarShuffleIndex[i] = G[jOTH];
      jOTH++;
    }
  }

  uint8_t cnt = 0;
  for (int i = 0; i < 32; ++i) {
    if (isScrambled[i] != 0) {
      seqShuffleIndex[cnt] = i;
      cnt++;
    }
  }

  __v2048i16 seqShuffleIndex_vec;
  vclaim(seqShuffleIndex_vec);
  int seqShuffleIndex_vec_addr = vaddr(seqShuffleIndex_vec);
  vbarrier();
  VSPM_OPEN();
  for (size_t i = 0; i < cnt; i++) {
    *(volatile short *)(seqShuffleIndex_vec_addr + (i << 1)) = seqShuffleIndex[i];
  }
  VSPM_CLOSE();

  __v2048i16 aBarShuffleIndex_vec;
  vclaim(aBarShuffleIndex_vec);
  int aBarShuffleIndex_vec_addr = vaddr(aBarShuffleIndex_vec);
  vbarrier();
  VSPM_OPEN();
  for (size_t i = 0; i < 32; i++) {
    *(volatile short *)(aBarShuffleIndex_vec_addr + (i << 1)) = aBarShuffleIndex[i];
  }
  VSPM_CLOSE();

  __v4096i8 seq;
  vclaim(seq);
  vbrdcst(seq, 0, MASKREAD_OFF, 32);
  vshuffle(seq, seqShuffleIndex_vec, seqSet_tmp, SHUFFLE_SCATTER, cnt);

  __v4096i8 trBlk;
  trBlk = vxor(scrBlk, seq, MASKREAD_OFF, 32);
  vshuffle(trBlk, aBarShuffleIndex_vec, trBlk, SHUFFLE_GATHER, 32);

  return trBlk;
}

typedef struct {
  short data;
} __attribute__((aligned(64))) short_struct;
int Task_nrPBCHBitProcess(__v4096i8 vin, __v2048i16 pi_, __v2048i16 Xor_up_index, __v2048i16 Xor_down_index,
                          __v2048i16 Equal_up_index, __v2048i16 Equal_down_index, __v4096i8 Mask_1010_table,
                          __v4096i8 Mask_0101_table, __v2048i16 index_front_135, __v2048i16 index_back_135,
                          __v2048i16 index_front_024, __v2048i16 index_back_024, __v4096i8 R_Matrix_0_up,
                          __v4096i8 R_Matrix_0_down, __v2048i16 PBCH_Info_index, __v2048i16 iIL_56_PBCH, __v4096i8 poly,
                          __v4096i8 seqSet, short_struct input_Lssb) {
  int E    = 864;
  int K    = 56;
  int N    = 512;
  int RNTI = 0;
  int Lssb = input_Lssb.data;

  __v4096i8 raterec = raterecover_polar_downlink(vin, E, K, N, 0, pi_);

  __v4096i8 decoded =
      polar_512bits(raterec, K, E, RNTI, 8, Xor_up_index, Xor_down_index, Equal_up_index, Equal_down_index,
                    Mask_1010_table, Mask_0101_table, index_front_135, index_back_135, index_front_024, index_back_024,
                    R_Matrix_0_up, R_Matrix_0_down, PBCH_Info_index, iIL_56_PBCH);

  __v4096i8 crcout = crc_24C(decoded, K, poly);

  short_struct crcBCH;
  short_struct sfn4lsb;
  short_struct nHalfFrame;
  short_struct msbidxoffset;

  crcBCH.data = 0;
  vbarrier();
  VSPM_OPEN();
  int decoded_addr = vaddr(decoded);
  int crcout_addr  = vaddr(crcout);
  for (int i = 0; i < 24; i++) {
    if (*(volatile char *)(decoded_addr + 32 + i) != *(volatile char *)(crcout_addr + i))
      crcBCH.data = 1;
  }
  VSPM_CLOSE();

  __v4096i8 scrBlk = DescrambleAndDeInterleaverIndex(decoded, seqSet, Lssb);

  int scrBlk_addr = vaddr(scrBlk);
  sfn4lsb.data    = 0;
  vbarrier();
  VSPM_OPEN();
  for (int i = 0; i < 4; ++i)
    sfn4lsb.data += (*(volatile char *)(scrBlk_addr + 24 + i)) << (4 - i - 1);
  VSPM_CLOSE();

  scrBlk_addr = vaddr(scrBlk);
  vbarrier();
  VSPM_OPEN();
  nHalfFrame.data = (*(volatile char *)(scrBlk_addr + 28));
  VSPM_CLOSE();

  scrBlk_addr = vaddr(scrBlk);
  vbarrier();
  VSPM_OPEN();
  msbidxoffset.data = (*(volatile char *)(scrBlk_addr + 29));
  VSPM_CLOSE();

  vreturn(scrBlk, 32, &crcBCH, sizeof(crcBCH), &sfn4lsb, sizeof(sfn4lsb), &nHalfFrame, sizeof(nHalfFrame),
          &msbidxoffset, sizeof(msbidxoffset));
}